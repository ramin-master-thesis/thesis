################ Document Stream Processing ################
@article{jugel2019fighting,
  title={Fighting Spam in Dating Apps},
  author={Jugel, Uwe and De Dios Santos, Juan and Trautmann, Evelyn and Behrens, Diogo},
  journal={BTW 2019},
  year={2019},
  publisher={Gesellschaft f{\"u}r Informatik, Bonn}
}
################ GraphJet ################
@article{sharma2016graphjet,
  title={GraphJet: Real-time content recommendations at Twitter},
  author={Sharma, Aneesh and Jiang, Jerry and Bommannavar, Praveen and Larson, Brian and Lin, Jimmy},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1281--1292},
  year={2016},
  publisher={VLDB Endowment}
}

@inproceedings{gupta2013wtf,
  title={Wtf: The who to follow service at twitter},
  author={Gupta, Pankaj and Goel, Ashish and Lin, Jimmy and Sharma, Aneesh and Wang, Dong and Zadeh, Reza},
  booktitle={Proceedings of the 22nd international conference on World Wide Web},
  pages={505--514},
  year={2013}
}

################ Salsa ################
@article{lempel2001salsa,
author = {Lempel, R. and Moran, S.},
title = {SALSA: The Stochastic Approach for Link-Structure Analysis},
year = {2001},
issue_date = {April 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/382979.383041},
doi = {10.1145/382979.383041},
abstract = {Today, when searching for information on the WWW, one usually performs a query through a term-based search engine. These engines return, as the query's result, a list of Web pages whose contents matches the query. For broad-topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. However, much information is contained in the link-structure of the WWW. Information such as which pages are linked to others can be used to augment search algorithms. In this context, Jon Kleinberg introduced the notion of two distinct types of Web pages: hubs and authorities. Kleinberg argued that hubs and authorities exhibit a mutually reinforcing relationship: a good hub will point to many  authorities, and a good authority will be pointed at by many hubs. In light of this, he dervised an algoirthm aimed at finding authoritative pages. We present SALSA, a new stochastic approach for link-structure analysis, which examines random walks on graphs derived from the link-structure. We show that both SALSA and Kleinberg's Mutual Reinforcement approach employ the same metaalgorithm.  We then prove that SALSA is quivalent to a weighted in degree analysis of the link-sturcutre of WWW subgraphs, making it computationally more efficient than the Mutual reinforcement approach. We compare that results of applying SALSA to the results derived through Kleinberg's approach. These comparisions reveal a topological Phenomenon called the TKC effectwhich, in certain cases,  prevents the Mutual reinforcement approach from identifying meaningful authorities.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
pages = {131–160},
numpages = {30},
keywords = {hubs and authorities, SALSA, random walks, TKC effect, Link-structure analysis}
}

################ Graph Embeddings ###############
@article{goyal2018graph,
  title={Graph embedding techniques, applications, and performance: A survey},
  author={Goyal, Palash and Ferrara, Emilio},
  journal={Knowledge-Based Systems},
  volume={151},
  pages={78--94},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{wang2016structural,
  title={Structural deep network embedding},
  author={Wang, Daixin and Cui, Peng and Zhu, Wenwu},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1225--1234},
  year={2016}
}


################ Embeddings ################
@article{StarSpace,
  author    = {Ledell Wu and
               Adam Fisch and
               Sumit Chopra and
               Keith Adams and
               Antoine Bordes and
               Jason Weston},
  title     = {StarSpace: Embed All The Things!},
  journal   = {CoRR},
  volume    = {abs/1709.03856},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.03856},
  archivePrefix = {arXiv},
  eprint    = {1709.03856},
  timestamp = {Mon, 13 Aug 2018 16:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-03856.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bengio2003,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@article{Collobert2011,
  author    = {Ronan Collobert and
               Jason Weston and
               L{\'{e}}on Bottou and
               Michael Karlen and
               Koray Kavukcuoglu and
               Pavel P. Kuksa},
  title     = {Natural Language Processing (almost) from Scratch},
  journal   = {CoRR},
  volume    = {abs/1103.0398},
  year      = {2011},
  url       = {http://arxiv.org/abs/1103.0398},
  archivePrefix = {arXiv},
  eprint    = {1103.0398},
  timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1103-0398.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{goldberg2014word2vec,
  title={word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
  author={Goldberg, Yoav and Levy, Omer},
  journal={arXiv preprint arXiv:1402.3722},
  year={2014}
}

@article{joulin2016fasttext,
  title={Fasttext. zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

################ Data Fusion ################
@article{hsu2005comparing,
  title={Comparing rank and score combination methods for data fusion in information retrieval},
  author={Hsu, D Frank and Taksa, Isak},
  journal={Information retrieval},
  volume={8},
  number={3},
  pages={449--480},
  year={2005},
  publisher={Springer}
}

@article{wu2006performance,
  title={Performance prediction of data fusion for information retrieval},
  author={Wu, Shengli and McClean, Sally},
  journal={Information processing \& management},
  volume={42},
  number={4},
  pages={899--915},
  year={2006},
  publisher={Elsevier}
}

