
@inproceedings{abdelhamidPartLyLearningData2020,
  title = {{{PartLy}}: Learning Data Partitioning for Distributed Data Stream Processing},
  shorttitle = {{{PartLy}}},
  booktitle = {Proceedings of the {{Third International Workshop}} on {{Exploiting Artificial Intelligence Techniques}} for {{Data Management}}},
  author = {Abdelhamid, Ahmed S. and Aref, Walid G.},
  year = {2020},
  month = jun,
  pages = {1--4},
  publisher = {{ACM}},
  address = {{Portland Oregon}},
  doi = {10.1145/3401071.3401660},
  isbn = {978-1-4503-8029-4},
  language = {en}
}

@inproceedings{baileyRetrievalConsistencyPresence2017,
  title = {Retrieval {{Consistency}} in the {{Presence}} of {{Query Variations}}},
  booktitle = {Proceedings of the 40th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Bailey, Peter and Moffat, Alistair and Scholer, Falk and Thomas, Paul},
  year = {2017},
  month = aug,
  pages = {395--404},
  publisher = {{ACM}},
  address = {{Shinjuku Tokyo Japan}},
  doi = {10.1145/3077136.3080839},
  isbn = {978-1-4503-5022-8},
  language = {en}
}

@inproceedings{balujaVideoSuggestionDiscovery2008,
  title = {Video Suggestion and Discovery for Youtube: Taking Random Walks through the View Graph},
  shorttitle = {Video Suggestion and Discovery for Youtube},
  booktitle = {Proceeding of the 17th International Conference on {{World Wide Web}}  - {{WWW}} '08},
  author = {Baluja, Shumeet and Seth, Rohan and Sivakumar, D. and Jing, Yushi and Yagnik, Jay and Kumar, Shankar and Ravichandran, Deepak and Aly, Mohamed},
  year = {2008},
  pages = {895},
  publisher = {{ACM Press}},
  address = {{Beijing, China}},
  doi = {10.1145/1367497.1367618},
  isbn = {978-1-60558-085-2},
  language = {en}
}

@article{bengioNeuralProbabilisticLanguage2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  year = {2003},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  issn = {1532-4435},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/home/ramin/Zotero/storage/4EU4UVL6/Bengio et al. - 2003 - A neural probabilistic language model.pdf}
}

@inproceedings{buckleyTopicPredictionBased2004,
  title = {Topic Prediction Based on Comparative Retrieval Rankings},
  booktitle = {Proceedings of the 27th Annual International Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '04},
  author = {Buckley, Chris},
  year = {2004},
  pages = {506},
  publisher = {{ACM Press}},
  address = {{Sheffield, United Kingdom}},
  doi = {10.1145/1008992.1009093},
  isbn = {978-1-58113-881-8},
  language = {en}
}

@inproceedings{chenRankingMeasuresLoss,
  title = {Ranking Measures and Loss Functions in Learning to Rank},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Chen, Wei and Liu, Tie-yan and Lan, Yanyan and Ma, Zhi-ming and Li, Hang},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {{Curran Associates, Inc.}}
}

@article{collobertNaturalLanguageProcessing2011,
  title = {Natural {{Language Processing}} (Almost) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  year = {2011},
  month = mar,
  journal = {arXiv:1103.0398 [cs]},
  eprint = {1103.0398},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ramin/Zotero/storage/43AAW44D/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf;/home/ramin/Zotero/storage/4TN3WHPQ/1103.html}
}

@article{cormodeImprovedDataStream2005,
  title = {An Improved Data Stream Summary: The Count-Min Sketch and Its Applications},
  shorttitle = {An Improved Data Stream Summary},
  author = {Cormode, Graham and Muthukrishnan, S.},
  year = {2005},
  month = apr,
  journal = {Journal of Algorithms},
  volume = {55},
  number = {1},
  pages = {58--75},
  issn = {01966774},
  doi = {10.1016/j.jalgor.2003.12.001},
  language = {en},
  file = {/home/ramin/Zotero/storage/Y9Z3XKA6/Cormode and Muthukrishnan - 2005 - An improved data stream summary the count-min ske.pdf}
}

@inproceedings{dasGoogleNewsPersonalization2007,
  title = {Google News Personalization: Scalable Online Collaborative Filtering},
  shorttitle = {Google News Personalization},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}  - {{WWW}} '07},
  author = {Das, Abhinandan S. and Datar, Mayur and Garg, Ashutosh and Rajaram, Shyam},
  year = {2007},
  pages = {271},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1242572.1242610},
  isbn = {978-1-59593-654-7},
  language = {en}
}

@article{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  journal = {Association for Computational Linguistics},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ramin/Zotero/storage/VQDZDJAB/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/home/ramin/Zotero/storage/Y9DFJRLJ/1810.html}
}

@inproceedings{drakopoulosUnsupervisedDiscoverySemantically2020,
  title = {Unsupervised {{Discovery Of Semantically Aware Communities With Tensor Kruskal Decomposition}}: {{A Case Study In Twitter}}},
  shorttitle = {Unsupervised {{Discovery Of Semantically Aware Communities With Tensor Kruskal Decomposition}}},
  booktitle = {2020 15th {{International Workshop}} on {{Semantic}} and {{Social Media Adaptation}} and {{Personalization}} ({{SMA}}},
  author = {Drakopoulos, Georgios and Giotopoulos, Konstantinos and Giannoukou, Ioanna and Sioutas, Spyros},
  year = {2020},
  month = oct,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Zakynthos, Greece}},
  doi = {10.1109/SMAP49528.2020.9248469},
  isbn = {978-1-72815-919-5}
}

@techreport{eastlakeUSSecureHash2001,
  title = {{{US Secure Hash Algorithm}} 1 ({{SHA1}})},
  author = {Eastlake, D. and Jones, P.},
  year = {2001},
  month = sep,
  number = {RFC3174},
  pages = {RFC3174},
  institution = {{RFC Editor}},
  doi = {10.17487/rfc3174},
  language = {en},
  file = {/home/ramin/Zotero/storage/498EANSQ/Eastlake and Jones - 2001 - US Secure Hash Algorithm 1 (SHA1).pdf}
}

@inproceedings{eksombatchaiPixieSystemRecommending2018,
  title = {Pixie: {{A System}} for {{Recommending}} 3+ {{Billion Items}} to 200+ {{Million Users}} in {{Real}}-{{Time}}},
  shorttitle = {Pixie},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}} - {{WWW}} '18},
  author = {Eksombatchai, Chantat and Jindal, Pranav and Liu, Jerry Zitao and Liu, Yuchen and Sharma, Rahul and Sugnet, Charles and Ulrich, Mark and Leskovec, Jure},
  year = {2018},
  pages = {1775--1784},
  publisher = {{ACM Press}},
  address = {{Lyon, France}},
  doi = {10.1145/3178876.3186183},
  isbn = {978-1-4503-5639-8},
  language = {en}
}

@article{fortunatoCommunityDetectionGraphs2010,
  title = {Community Detection in Graphs},
  author = {Fortunato, Santo},
  year = {2010},
  month = feb,
  journal = {Physics Reports},
  volume = {486},
  number = {3-5},
  pages = {75--174},
  issn = {03701573},
  doi = {10.1016/j.physrep.2009.11.002},
  language = {en},
  file = {/home/ramin/Zotero/storage/SUP4IGMV/Fortunato - 2010 - Community detection in graphs.pdf}
}

@article{frankhsuComparingRankScore2005,
  title = {Comparing {{Rank}} and {{Score Combination Methods}} for {{Data Fusion}} in {{Information Retrieval}}},
  author = {Frank Hsu, D. and Taksa, Isak},
  year = {2005},
  month = jan,
  journal = {Information Retrieval},
  volume = {8},
  number = {3},
  pages = {449--480},
  issn = {1386-4564, 1573-7659},
  doi = {10.1007/s10791-005-6994-4},
  language = {en}
}

@article{goelWhoToFollowSystemTwitter2015,
  title = {The {{Who}}-{{To}}-{{Follow System}} at {{Twitter}}: {{Strategy}}, {{Algorithms}}, and {{Revenue Impact}}},
  shorttitle = {The {{Who}}-{{To}}-{{Follow System}} at {{Twitter}}},
  author = {Goel, Ashish and Gupta, Pankaj and Sirois, John and Wang, Dong and Sharma, Aneesh and Gurumurthy, Siva},
  year = {2015},
  month = feb,
  journal = {Interfaces},
  volume = {45},
  number = {1},
  pages = {98--107},
  issn = {0092-2102, 1526-551X},
  doi = {10.1287/inte.2014.0784},
  language = {en}
}

@article{goelWhoToFollowSystemTwitter2015a,
  title = {The {{Who}}-{{To}}-{{Follow System}} at {{Twitter}}: {{Strategy}}, {{Algorithms}}, and {{Revenue Impact}}},
  shorttitle = {The {{Who}}-{{To}}-{{Follow System}} at {{Twitter}}},
  author = {Goel, Ashish and Gupta, Pankaj and Sirois, John and Wang, Dong and Sharma, Aneesh and Gurumurthy, Siva},
  year = {2015},
  month = feb,
  journal = {Interfaces},
  volume = {45},
  number = {1},
  pages = {98--107},
  issn = {0092-2102, 1526-551X},
  doi = {10.1287/inte.2014.0784},
  language = {en}
}

@article{goldbergUsingCollaborativeFiltering1992,
  title = {Using Collaborative Filtering to Weave an Information Tapestry},
  author = {Goldberg, David and Nichols, David and Oki, Brian M. and Terry, Douglas},
  year = {1992},
  month = dec,
  journal = {Communications of the ACM},
  volume = {35},
  number = {12},
  pages = {61--70},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/138859.138867},
  language = {en}
}

@article{goldbergWord2vecExplainedDeriving2014,
  title = {Word2vec {{Explained}}: Deriving {{Mikolov}} et al.'s Negative-Sampling Word-Embedding Method},
  shorttitle = {Word2vec {{Explained}}},
  author = {Goldberg, Yoav and Levy, Omer},
  year = {2014},
  month = feb,
  journal = {arXiv:1402.3722 [cs, stat]},
  eprint = {1402.3722},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ramin/Zotero/storage/RTFF23ED/Goldberg and Levy - 2014 - word2vec Explained deriving Mikolov et al.'s nega.pdf;/home/ramin/Zotero/storage/ADH4ECKJ/1402.html}
}

@incollection{gontarskaPredictingMedicalInterventions2021,
  title = {Predicting {{Medical Interventions}} from {{Vital Parameters}}: {{Towards}} a {{Decision Support System}} for {{Remote Patient Monitoring}}},
  shorttitle = {Predicting {{Medical Interventions}} from {{Vital Parameters}}},
  booktitle = {Artificial {{Intelligence}} in {{Medicine}}},
  author = {Gontarska, Kordian and Wrazen, Weronika and Beilharz, Jossekin and Schmid, Robert and Thamsen, Lauritz and Polze, Andreas},
  editor = {Tucker, Allan and Henriques Abreu, Pedro and Cardoso, Jaime and Pereira Rodrigues, Pedro and Ria{\~n}o, David},
  year = {2021},
  volume = {12721},
  pages = {293--297},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-77211-6_33},
  isbn = {978-3-030-77210-9 978-3-030-77211-6},
  language = {en},
  file = {/home/ramin/Zotero/storage/KLNIENYX/Gontarska et al. - 2021 - Predicting Medical Interventions from Vital Parame.pdf}
}

@article{gontarskaPredictingMedicalInterventions2021a,
  title = {Predicting {{Medical Interventions}} from {{Vital Parameters}}: {{Towards}} a {{Decision Support System}} for {{Remote Patient Monitoring}}},
  shorttitle = {Predicting {{Medical Interventions}} from {{Vital Parameters}}},
  author = {Gontarska, Kordian and Wrazen, Weronika and Beilharz, Jossekin and Schmid, Robert and Thamsen, Lauritz and Polze, Andreas},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.10085 [cs]},
  eprint = {2104.10085},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Cardiovascular diseases and heart failures in particular are the main cause of non-communicable disease mortality in the world. Constant patient monitoring enables better medical treatment as it allows practitioners to react on time and provide the appropriate treatment. Telemedicine can provide constant remote monitoring so patients can stay in their homes, only requiring medical sensing equipment and network connections. A limiting factor for telemedical centers is the amount of patients that can be monitored simultaneously. We aim to increase this amount by implementing a decision support system. This paper investigates a machine learning model to estimate a risk score based on patient vital parameters that allows sorting all cases every day to help practitioners focus their limited capacities on the most severe cases. The model we propose reaches an AUCROC of 0.84, whereas the baseline rule-based model reaches an AUCROC of 0.73. Our results indicate that the usage of deep learning to improve the efficiency of telemedical centers is feasible. This way more patients could benefit from better health-care through remote monitoring.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/ramin/Zotero/storage/9KHIYQD7/Gontarska et al. - 2021 - Predicting Medical Interventions from Vital Parame.pdf;/home/ramin/Zotero/storage/5BLWZ9G7/2104.html}
}

@article{goyalGraphEmbeddingTechniques2018,
  title = {Graph Embedding Techniques, Applications, and Performance: {{A}} Survey},
  shorttitle = {Graph Embedding Techniques, Applications, and Performance},
  author = {Goyal, Palash and Ferrara, Emilio},
  year = {2018},
  month = jul,
  journal = {Knowledge-Based Systems},
  volume = {151},
  pages = {78--94},
  issn = {09507051},
  doi = {10.1016/j.knosys.2018.03.022},
  language = {en},
  file = {/home/ramin/Zotero/storage/DB9I9MCJ/Goyal and Ferrara - 2018 - Graph embedding techniques, applications, and perf.pdf}
}

@inproceedings{grewalEvolutionContentAnalysis2018,
  title = {The {{Evolution}} of {{Content Analysis}} for {{Personalized Recommendations}} at {{Twitter}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Grewal, Ajeet and Lin, Jimmy},
  year = {2018},
  month = jun,
  pages = {1355--1356},
  publisher = {{ACM}},
  address = {{Ann Arbor MI USA}},
  doi = {10.1145/3209978.3210206},
  isbn = {978-1-4503-5657-2},
  language = {en}
}

@inproceedings{grewalRecServiceDistributedRealTime2018,
  title = {{{RecService}}: {{Distributed Real}}-{{Time Graph Processing}} at {{Twitter}}},
  shorttitle = {{{RecService}}},
  booktitle = {10th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Workshop}} on {{Hot Topics}} in {{Cloud Computing}} ({{HotCloud}} 18)},
  author = {Grewal, Ajeet and Jiang, Jerry and Lam, Gary and Jung, Tristan and Vuddemarri, Lohith and Li, Quannan and Landge, Aaditya and Lin, Jimmy},
  year = {2018},
  language = {en},
  file = {/home/ramin/Zotero/storage/DJ68P397/Grewal et al. - 2018 - RecService Distributed Real-Time Graph Processing.pdf;/home/ramin/Zotero/storage/LI38CCLQ/grewal.html}
}

@inproceedings{groverNode2vecScalableFeature2016,
  title = {Node2vec: {{Scalable Feature Learning}} for {{Networks}}},
  shorttitle = {Node2vec},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Grover, Aditya and Leskovec, Jure},
  year = {2016},
  month = aug,
  pages = {855--864},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939754},
  isbn = {978-1-4503-4232-2},
  language = {en},
  file = {/home/ramin/Zotero/storage/2NFQ8I7X/Grover and Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf}
}

@article{gunawardanaSurveyAccuracyEvaluation,
  title = {A {{Survey}} of {{Accuracy Evaluation Metrics}} of {{Recommendation Tasks}}},
  author = {Gunawardana, Asela and Shani, Guy},
  pages = {28},
  abstract = {Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms offline using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of offline experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing offline experiments.},
  language = {en},
  file = {/home/ramin/Zotero/storage/D2M78N6S/Gunawardana and Shani - A Survey of Accuracy Evaluation Metrics of Recomme.pdf}
}

@article{gunawardanaSurveyAccuracyEvaluationa,
  title = {A {{Survey}} of {{Accuracy Evaluation Metrics}} of {{Recommendation Tasks}}},
  author = {Gunawardana, Asela and Shani, Guy},
  pages = {28},
  abstract = {Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms offline using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of offline experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing offline experiments.},
  language = {en},
  file = {/home/ramin/Zotero/storage/TZH8DABG/Gunawardana and Shani - A Survey of Accuracy Evaluation Metrics of Recomme.pdf}
}

@inproceedings{guptaWTFWhoFollow2013,
  title = {{{WTF}}: The Who to Follow Service at {{Twitter}}},
  shorttitle = {{{WTF}}},
  booktitle = {Proceedings of the 22nd International Conference on {{World Wide Web}} - {{WWW}} '13},
  author = {Gupta, Pankaj and Goel, Ashish and Lin, Jimmy and Sharma, Aneesh and Wang, Dong and Zadeh, Reza},
  year = {2013},
  pages = {505--514},
  publisher = {{ACM Press}},
  address = {{Rio de Janeiro, Brazil}},
  doi = {10.1145/2488388.2488433},
  isbn = {978-1-4503-2035-1},
  language = {en}
}

@article{herlockerEvaluatingCollaborativeFiltering2004,
  title = {Evaluating Collaborative Filtering Recommender Systems},
  author = {Herlocker, Jonathan L. and Konstan, Joseph A. and Terveen, Loren G. and Riedl, John T.},
  year = {2004},
  month = jan,
  journal = {ACM Transactions on Information Systems},
  volume = {22},
  number = {1},
  pages = {5--53},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/963770.963772},
  abstract = {Recommender systems have been evaluated in many, often incomparable, ways. In this article, we review the key decisions in evaluating collaborative filtering recommender systems: the user tasks being evaluated, the types of analysis and datasets being used, the ways in which prediction quality is measured, the evaluation of prediction attributes other than quality, and the user-based evaluation of the system as a whole. In addition to reviewing the evaluation strategies used by prior researchers, we present empirical results from the analysis of various accuracy metrics on one content domain where all the tested metrics collapsed roughly into three equivalence classes. Metrics within each equivalency class were strongly correlated, while metrics from different equivalency classes were uncorrelated.},
  language = {en}
}

@article{hollocouStreamingAlgorithmGraph2017,
  title = {A {{Streaming Algorithm}} for {{Graph Clustering}}},
  author = {Hollocou, Alexandre and Maudet, Julien and Bonald, Thomas and Lelarge, Marc},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.04337 [cs]},
  eprint = {1712.04337},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a novel algorithm to perform graph clustering in the edge streaming setting. In this model, the graph is presented as a sequence of edges that can be processed strictly once. Our streaming algorithm has an extremely low memory footprint as it stores only three integers per node and does not keep any edge in memory. We provide a theoretical justification of the design of the algorithm based on the modularity function, which is a usual metric to evaluate the quality of a graph partition. We perform experiments on massive real-life graphs ranging from one million to more than one billion edges and we show that this new algorithm runs more than ten times faster than existing algorithms and leads to similar or better detection scores on the largest graphs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  note = {Comment: NIPS Wokshop on Advances in Modeling and Learning Interactions from Complex Data, 2017. arXiv admin note: substantial text overlap with arXiv:1703.02955},
  file = {/home/ramin/Zotero/storage/TVEKLRWP/Hollocou et al. - 2017 - A Streaming Algorithm for Graph Clustering.pdf;/home/ramin/Zotero/storage/JXJ49WVE/1712.html}
}

@article{jarvelinCumulatedGainbasedEvaluation2002,
  title = {Cumulated Gain-Based Evaluation of {{IR}} Techniques},
  author = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  year = {2002},
  month = oct,
  journal = {ACM Transactions on Information Systems},
  volume = {20},
  number = {4},
  pages = {422--446},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/582415.582418},
  abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
  language = {en}
}

@article{jolliffePrincipalComponentAnalysis2016,
  title = {Principal Component Analysis: A Review and Recent Developments},
  shorttitle = {Principal Component Analysis},
  author = {Jolliffe, Ian T. and Cadima, Jorge},
  year = {2016},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {374},
  number = {2065},
  pages = {20150202},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2015.0202},
  abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not               a priori               , hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
  language = {en},
  file = {/home/ramin/Zotero/storage/25IN26DC/Jolliffe and Cadima - 2016 - Principal component analysis a review and recent .pdf}
}

@article{joulinFastTextZipCompressing2016,
  title = {{{FastText}}.Zip: {{Compressing}} Text Classification Models},
  shorttitle = {{{FastText}}.Zip},
  author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.03651 [cs]},
  eprint = {1612.03651},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Submitted to ICLR 2017},
  file = {/home/ramin/Zotero/storage/94I54EDM/Joulin et al. - 2016 - FastText.zip Compressing text classification mode.pdf;/home/ramin/Zotero/storage/4942CHPM/1612.html}
}

@inproceedings{jrvelinIREvaluationMethods2000,
  title = {{{IR}} Evaluation Methods for Retrieving Highly Relevant Documents},
  booktitle = {Proceedings of the 23rd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '00},
  author = {J?rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  year = {2000},
  pages = {41--48},
  publisher = {{ACM Press}},
  address = {{Athens, Greece}},
  doi = {10.1145/345508.345545},
  isbn = {978-1-58113-226-7},
  language = {en}
}

@article{kendallNewMeasureRank1938,
  title = {A {{New Measure}} of {{Rank Correlation}}},
  author = {Kendall, M. G.},
  year = {1938},
  month = jun,
  journal = {Biometrika},
  volume = {30},
  number = {1/2},
  pages = {81},
  issn = {00063444},
  doi = {10.2307/2332226}
}

@article{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  journal = {arXiv:1609.02907 [cs, stat]},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2017},
  file = {/home/ramin/Zotero/storage/TBGVT3PC/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/ramin/Zotero/storage/2TY4QNXB/1609.html}
}

@article{kramerNonlinearPrincipalComponent1991,
  title = {Nonlinear Principal Component Analysis Using Autoassociative Neural Networks},
  author = {Kramer, Mark A.},
  year = {1991},
  month = feb,
  journal = {AIChE Journal},
  volume = {37},
  number = {2},
  pages = {233--243},
  issn = {0001-1541, 1547-5905},
  doi = {10.1002/aic.690370209},
  language = {en}
}

@inproceedings{kutzkovWeightedSimilarityEstimation2015,
  title = {Weighted {{Similarity Estimation}} in {{Data Streams}}},
  booktitle = {Proceedings of the 24th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Kutzkov, Konstantin and Ahmed, Mohamed and Nikitaki, Sofia},
  year = {2015},
  month = oct,
  pages = {1051--1060},
  publisher = {{ACM}},
  address = {{Melbourne Australia}},
  doi = {10.1145/2806416.2806515},
  isbn = {978-1-4503-3794-6},
  language = {en}
}

@inproceedings{kwakWhatTwitterSocial2010,
  title = {What Is {{Twitter}}, a Social Network or a News Media?},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  author = {Kwak, Haewoon and Lee, Changhyun and Park, Hosung and Moon, Sue},
  year = {2010},
  pages = {591},
  publisher = {{ACM Press}},
  address = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/1772690.1772751},
  isbn = {978-1-60558-799-8},
  language = {en}
}

@article{lempelSALSAStochasticApproach2001,
  title = {{{SALSA}}: The Stochastic Approach for Link-Structure Analysis},
  shorttitle = {{{SALSA}}},
  author = {Lempel, R. and Moran, S.},
  year = {2001},
  month = apr,
  journal = {ACM Transactions on Information Systems},
  volume = {19},
  number = {2},
  pages = {131--160},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/382979.383041},
  abstract = {Today, when searching for information on the WWW, one usually performs a query through a term-based search engine. These engines return, as the query's result, a list of Web pages whose contents matches the query. For broad-topic queries, such searches often result in a huge set of retrieved documents, many of which are irrelevant to the user. However, much information is contained in the link-structure of the WWW. Information such as which pages are linked to others can be used to augment search algorithms. In this context, Jon Kleinberg introduced the notion of two distinct types of Web pages:               hubs               and               authorities               . Kleinberg argued that hubs and authorities exhibit a               mutually reinforcing relationship               : a good hub will point to many  authorities, and a good authority will be pointed at by many hubs. In light of this, he dervised an algoirthm aimed at finding authoritative pages. We present SALSA, a new stochastic approach for link-structure analysis, which examines random walks on graphs derived from the link-structure. We show that both SALSA and Kleinberg's Mutual Reinforcement approach employ the same metaalgorithm.  We then prove that SALSA is quivalent to a weighted in degree analysis of the link-sturcutre of WWW subgraphs, making it computationally more efficient than the Mutual reinforcement approach. We compare that results of applying SALSA to the results derived through Kleinberg's approach. These comparisions reveal a topological Phenomenon called the TKC               effect               which, in certain cases,  prevents the Mutual reinforcement approach from identifying meaningful authorities.},
  language = {en}
}

@article{liakosRapidDetectionLocal2020,
  title = {Rapid {{Detection}} of {{Local Communities}} in {{Graph Streams}}},
  author = {Liakos, Panagiotis and Papakonstantinopoulou, Katia and Ntoulas, Alexandros and Delis, Alex},
  year = {2020},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2020.3012608}
}

@article{lindenAmazonComRecommendations2003,
  title = {Amazon.Com Recommendations: Item-to-Item Collaborative Filtering},
  shorttitle = {Amazon.Com Recommendations},
  author = {Linden, G. and Smith, B. and York, J.},
  year = {2003},
  month = jan,
  journal = {IEEE Internet Computing},
  volume = {7},
  number = {1},
  pages = {76--80},
  issn = {1089-7801},
  doi = {10.1109/MIC.2003.1167344},
  language = {en}
}

@inproceedings{liuFilmTVActors2018,
  title = {Film and {{TV Actors Recommendation Based}} on {{SALSA Algorithm}}},
  booktitle = {2018 {{IEEE}}/{{ACIS}} 17th {{International Conference}} on {{Computer}} and {{Information Science}} ({{ICIS}})},
  author = {Liu, Xingyan and Li, Chunfang and {Aimoerfu} and Wu, Dianzhao},
  year = {2018},
  month = jun,
  pages = {372--376},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICIS.2018.8466377},
  isbn = {978-1-5386-5892-5}
}

@article{mahmudSurveyDataPartitioning2020,
  title = {A Survey of Data Partitioning and Sampling Methods to Support Big Data Analysis},
  author = {Mahmud, Mohammad Sultan and Huang, Joshua Zhexue and Salloum, Salman and Emara, Tamer Z. and Sadatdiynov, Kuanishbay},
  year = {2020},
  month = jun,
  journal = {Big Data Mining and Analytics},
  volume = {3},
  number = {2},
  pages = {85--101},
  issn = {2096-0654},
  doi = {10.26599/BDMA.2019.9020015},
  file = {/home/ramin/Zotero/storage/TAVEK6TY/Mahmud et al. - 2020 - A survey of data partitioning and sampling methods.pdf}
}

@book{manningIntroductionInformationRetrieval2008,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  isbn = {978-0-521-86571-5},
  lccn = {QA76.9.T48 M26 2008},
  keywords = {Document clustering,Information retrieval,Semantic Web,Text processing (Computer science)},
  annotation = {OCLC: ocn190786122}
}

@article{martinezPCALDA2001,
  title = {{{PCA}} versus {{LDA}}},
  author = {Martinez, A.M. and Kak, A.C.},
  year = {Feb./2001},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {23},
  number = {2},
  pages = {228--233},
  issn = {01628828},
  doi = {10.1109/34.908974}
}

@inproceedings{mclaughlinCollaborativeFilteringAlgorithm2004,
  title = {A Collaborative Filtering Algorithm and Evaluation Metric That Accurately Model the User Experience},
  booktitle = {Proceedings of the 27th Annual International Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '04},
  author = {McLaughlin, Matthew R. and Herlocker, Jonathan L.},
  year = {2004},
  pages = {329},
  publisher = {{ACM Press}},
  address = {{Sheffield, United Kingdom}},
  doi = {10.1145/1008992.1009050},
  isbn = {978-1-58113-881-8},
  language = {en}
}

@inproceedings{miettinenModelOrderSelection2011,
  title = {Model Order Selection for Boolean Matrix Factorization},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '11},
  author = {Miettinen, Pauli and Vreeken, Jilles},
  year = {2011},
  pages = {51},
  publisher = {{ACM Press}},
  address = {{San Diego, California, USA}},
  doi = {10.1145/2020408.2020424},
  isbn = {978-1-4503-0813-7},
  language = {en},
  file = {/home/ramin/Zotero/storage/9FVL5VUL/Miettinen and Vreeken - 2011 - Model order selection for boolean matrix factoriza.pdf}
}

@article{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  journal = {arXiv:1301.3781 [cs]},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ramin/Zotero/storage/UEKR7AFH/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/home/ramin/Zotero/storage/BWNRR4IM/1301.html}
}

@article{neumannBiclusteringBooleanMatrix2020,
  title = {Biclustering and Boolean Matrix Factorization in Data Streams},
  author = {Neumann, Stefan and Miettinen, Pauli},
  year = {2020},
  month = jun,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {10},
  pages = {1709--1722},
  issn = {2150-8097},
  doi = {10.14778/3401960.3401968},
  abstract = {We study clustering of bipartite graphs and Boolean matrix factorization in data streams. We consider a streaming setting in which the vertices from the               left               side of the graph arrive one by one together with all of their incident edges. We provide an algorithm which after one pass over the stream recovers the set of clusters on the               right               side of the graph using sublinear space; to the best of our knowledge this is the first algorithm with this property. We also show that after a second pass over the stream the left clusters of the bipartite graph can be recovered and we show how to extend our algorithm to solve the Boolean matrix factorization problem (by exploiting the correspondence of Boolean matrices and bipartite graphs). We evaluate an implementation of the algorithm on synthetic data and on real-world data. On real-world datasets the algorithm is orders of magnitudes faster than a static baseline algorithm while providing quality results within a factor 2 of the baseline algorithm. Our algorithm scales linearly in the number of edges in the graph. Finally, we analyze the algorithm theoretically and provide sufficient conditions under which the algorithm recovers a set of planted clusters under a standard random graph model.},
  language = {en},
  file = {/home/ramin/Zotero/storage/GSMVWD2L/Neumann and Miettinen - 2020 - Biclustering and boolean matrix factorization in d.pdf}
}

@inproceedings{ouAsymmetricTransitivityPreserving2016b,
  title = {Asymmetric {{Transitivity Preserving Graph Embedding}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ou, Mingdong and Cui, Peng and Pei, Jian and Zhang, Ziwei and Zhu, Wenwu},
  year = {2016},
  month = aug,
  pages = {1105--1114},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939751},
  isbn = {978-1-4503-4232-2},
  language = {en}
}

@inproceedings{perozziDeepWalkOnlineLearning2014,
  title = {{{DeepWalk}}: Online Learning of Social Representations},
  shorttitle = {{{DeepWalk}}},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Perozzi, Bryan and {Al-Rfou}, Rami and Skiena, Steven},
  year = {2014},
  month = aug,
  pages = {701--710},
  publisher = {{ACM}},
  address = {{New York New York USA}},
  doi = {10.1145/2623330.2623732},
  isbn = {978-1-4503-2956-9},
  language = {en},
  file = {/home/ramin/Zotero/storage/R68N68YT/Perozzi et al. - 2014 - DeepWalk online learning of social representation.pdf}
}

@article{resnickRecommenderSystems1997a,
  title = {Recommender Systems},
  author = {Resnick, Paul and Varian, Hal R.},
  year = {1997},
  month = mar,
  journal = {Communications of the ACM},
  volume = {40},
  number = {3},
  pages = {56--58},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/245108.245121},
  language = {en}
}

@techreport{rivestMD5MessageDigestAlgorithm1992,
  title = {The {{MD5 Message}}-{{Digest Algorithm}}},
  author = {Rivest, R.},
  year = {1992},
  month = apr,
  number = {RFC1321},
  pages = {RFC1321},
  institution = {{RFC Editor}},
  doi = {10.17487/rfc1321},
  language = {en},
  file = {/home/ramin/Zotero/storage/YF7E7P5L/Rivest - 1992 - The MD5 Message-Digest Algorithm.pdf}
}

@book{sammutEncyclopediaMachineLearning2010,
  title = {Encyclopedia of Machine Learning},
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  year = {2010},
  publisher = {{Springer}},
  address = {{New York ; London}},
  isbn = {978-0-387-30768-8 978-0-387-34558-1 978-0-387-30164-8},
  lccn = {Q325.5 .E53 2010},
  keywords = {Encyclopedias,Machine learning},
  annotation = {OCLC: ocn651073009}
}

@incollection{sandersDistributedEvolutionaryGraph2012,
  title = {Distributed {{Evolutionary Graph Partitioning}}},
  booktitle = {2012 {{Proceedings}} of the {{Fourteenth Workshop}} on {{Algorithm Engineering}} and {{Experiments}} ({{ALENEX}})},
  author = {Sanders, Peter and Schulz, Christian},
  editor = {Bader, David A. and Mutzel, Dort},
  year = {2012},
  month = jan,
  pages = {16--29},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  doi = {10.1137/1.9781611972924.2},
  isbn = {978-1-61197-212-2 978-1-61197-292-4},
  language = {en},
  file = {/home/ramin/Zotero/storage/IQEKTECP/Sanders and Schulz - 2012 - Distributed Evolutionary Graph Partitioning.pdf}
}

@inproceedings{satuluriSimClustersCommunityBasedRepresentations2020,
  title = {{{SimClusters}}: {{Community}}-{{Based Representations}} for {{Heterogeneous Recommendations}} at {{Twitter}}},
  shorttitle = {{{SimClusters}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Satuluri, Venu and Wu, Yao and Zheng, Xun and Qian, Yilei and Wichers, Brian and Dai, Qieyun and Tang, Gui Ming and Jiang, Jerry and Lin, Jimmy},
  year = {2020},
  month = aug,
  pages = {3183--3193},
  publisher = {{ACM}},
  address = {{Virtual Event CA USA}},
  doi = {10.1145/3394486.3403370},
  isbn = {978-1-4503-7998-4},
  language = {en}
}

@inproceedings{schaferMetarecommendationSystemsUsercontrolled2002,
  title = {Meta-Recommendation Systems: User-Controlled Integration of Diverse Recommendations},
  shorttitle = {Meta-Recommendation Systems},
  booktitle = {Proceedings of the Eleventh International Conference on {{Information}} and Knowledge Management - {{CIKM}} '02},
  author = {Schafer, J. Ben and Konstan, Joseph A. and Riedl, John},
  year = {2002},
  pages = {43--51},
  publisher = {{ACM Press}},
  address = {{McLean, Virginia, USA}},
  doi = {10.1145/584792.584803},
  isbn = {978-1-58113-492-6},
  language = {en}
}

@article{sharmaGraphJetRealtimeContent2016,
  title = {{{GraphJet}}: Real-Time Content Recommendations at Twitter},
  shorttitle = {{{GraphJet}}},
  author = {Sharma, Aneesh and Jiang, Jerry and Bommannavar, Praveen and Larson, Brian and Lin, Jimmy},
  year = {2016},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {13},
  pages = {1281--1292},
  issn = {2150-8097},
  doi = {10.14778/3007263.3007267},
  abstract = {This paper presents GraphJet, a new graph-based system for generating content recommendations at Twitter. As motivation, we trace the evolution of our formulation and approach to the graph recommendation problem, embodied in successive generations of systems. Two trends can be identified: supplementing batch with real-time processing and a broadening of the scope of recommendations from users to content. Both of these trends come together in Graph-Jet, an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements a simple API, but one that is sufficiently expressive to support a range of recommendation algorithms based on random walks that we have refined over the years. Similar to Cassovary, a previous graph recommendation engine developed at Twitter, GraphJet assumes that the entire graph can be held in memory on a single server. The system organizes the interaction graph into temporally-partitioned index segments that hold adjacency lists. GraphJet is able to support rapid ingestion of edges while concurrently serving lookup queries through a combination of compact edge encoding and a dynamic memory allocation scheme that exploits power-law characteristics of the graph. Each GraphJet server ingests up to one million graph edges per second, and in steady state, computes up to 500 recommendations per second, which translates into several million edge read operations per second.},
  language = {en}
}

@book{skienaImplementingDiscreteMathematics1990,
  title = {Implementing Discrete Mathematics: Combinatorics and Graph Theory with {{Mathematica}}},
  shorttitle = {Implementing Discrete Mathematics},
  author = {Skiena, Steven S.},
  year = {1990},
  publisher = {{Addison-Wesley}},
  address = {{Redwood City, Calif}},
  isbn = {978-0-201-50943-4},
  lccn = {QA164 .S56 1990},
  keywords = {Combinatorial analysis,Data processing,Graph theory,Mathematica (Computer file)}
}

@book{skienaImplementingDiscreteMathematics1990a,
  title = {Implementing Discrete Mathematics: Combinatorics and Graph Theory with {{Mathematica}}},
  shorttitle = {Implementing Discrete Mathematics},
  author = {Skiena, Steven S.},
  year = {1990},
  publisher = {{Addison-Wesley}},
  address = {{Redwood City, Calif}},
  isbn = {978-0-201-50943-4},
  lccn = {QA164 .S56 1990},
  keywords = {Combinatorial analysis,Data processing,Graph theory,Mathematica (Computer file)}
}

@article{slotaPartitioningTrillionedgeGraphs2016,
  title = {Partitioning {{Trillion}}-Edge {{Graphs}} in {{Minutes}}},
  author = {Slota, George M. and Rajamanickam, Sivasankaran and Devine, Karen and Madduri, Kamesh},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.07220 [cs]},
  eprint = {1610.07220},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce XtraPuLP, a new distributed-memory graph partitioner designed to process trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated as a viable means to produce high quality partitions with minimal computation time. On a collection of large sparse graphs, we show that XtraPuLP partitioning quality is comparable to state-of-the-art partitioning methods. We also demonstrate that XtraPuLP can produce partitions of real-world graphs with billion+ vertices in minutes. Further, we show that using XtraPuLP partitions for distributed-memory graph analytics leads to significant end-to-end execution time reduction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/ramin/Zotero/storage/MNYA52S5/Slota et al. - 2016 - Partitioning Trillion-edge Graphs in Minutes.pdf;/home/ramin/Zotero/storage/BWULYDE4/1610.html}
}

@article{suSurveyCollaborativeFiltering2009,
  title = {A {{Survey}} of {{Collaborative Filtering Techniques}}},
  author = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
  year = {2009},
  month = oct,
  journal = {Advances in Artificial Intelligence},
  volume = {2009},
  pages = {1--19},
  issn = {1687-7470, 1687-7489},
  doi = {10.1155/2009/421425},
  abstract = {As one of the most successful approaches to building recommender systems, collaborative filtering (               CF               ) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.},
  language = {en},
  file = {/home/ramin/Zotero/storage/JLMXEAIR/Su and Khoshgoftaar - 2009 - A Survey of Collaborative Filtering Techniques.pdf}
}

@article{takacsScalableCollaborativeFiltering,
  title = {Scalable {{Collaborative Filtering Approaches}} for {{Large Recommender Systems}}},
  author = {Takacs, Gabor and Pilaszy, Istvan and Nemeth, Bottyan and Tikk, Domonkos},
  pages = {34},
  abstract = {The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netflix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efficiently. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets.},
  language = {en},
  file = {/home/ramin/Zotero/storage/ITZY8TX4/Takacs et al. - Scalable Collaborative Filtering Approaches for La.pdf}
}

@article{takacsScalableCollaborativeFiltering2009,
  title = {Scalable {{Collaborative Filtering Approaches}} for {{Large Recommender Systems}}},
  author = {Tak{\'a}cs, G{\'a}bor and Pil{\'a}szy, Istv{\'a}n and N{\'e}meth, Botty{\'a}n and Tikk, Domonkos},
  year = {2009},
  month = jun,
  journal = {The Journal of Machine Learning Research},
  volume = {10},
  pages = {623--656},
  issn = {1532-4435},
  abstract = {The collaborative filtering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subfield of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netflix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netflix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efficiently. In the experimentation section, we first report on some implementation issues, and we suggest on how parameter optimization can be performed efficiently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets.},
  file = {/home/ramin/Zotero/storage/T34JDKLK/Takács et al. - 2009 - Scalable Collaborative Filtering Approaches for La.pdf}
}

@article{tianLearningDeepRepresentations2014,
  title = {Learning {{Deep Representations}} for {{Graph Clustering}}},
  author = {Tian, Fei and Gao, Bin and Cui, Qing and Chen, Enhong and Liu, Tie-Yan},
  year = {2014},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {28},
  number = {1},
  issn = {2374-3468},
  copyright = {Copyright (c)},
  language = {en},
  keywords = {neural networks},
  file = {/home/ramin/Zotero/storage/JS84VDCD/Tian et al. - 2014 - Learning Deep Representations for Graph Clustering.pdf}
}

@article{vandermaaten08,
  title = {Visualizing Data Using T-{{SNE}}},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605}
}

@article{vanloanGeneralizingSingularValue1976,
  title = {Generalizing the {{Singular Value Decomposition}}},
  author = {Van Loan, Charles F.},
  year = {1976},
  month = mar,
  journal = {SIAM Journal on Numerical Analysis},
  volume = {13},
  number = {1},
  pages = {76--83},
  issn = {0036-1429, 1095-7170},
  doi = {10.1137/0713009},
  language = {en}
}

@inproceedings{wangStructuralDeepNetwork2016,
  title = {Structural {{Deep Network Embedding}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Wang, Daixin and Cui, Peng and Zhu, Wenwu},
  year = {2016},
  month = aug,
  pages = {1225--1234},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939753},
  isbn = {978-1-4503-4232-2},
  language = {en}
}

@article{webberSimilarityMeasureIndefinite2010,
  title = {A Similarity Measure for Indefinite Rankings},
  author = {Webber, William and Moffat, Alistair and Zobel, Justin},
  year = {2010},
  month = nov,
  journal = {ACM Transactions on Information Systems},
  volume = {28},
  number = {4},
  pages = {1--38},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/1852102.1852106},
  abstract = {Ranked lists are encountered in research and daily life and it is often of interest to compare these lists even when they are incomplete or have only some members in common. An example is document rankings returned for the same query by different search engines. A measure of the similarity between incomplete rankings should handle nonconjointness, weight high ranks more heavily than low, and be monotonic with increasing depth of evaluation; but no measure satisfying all these criteria currently exists. In this article, we propose a new measure having these qualities, namely rank-biased overlap (RBO). The RBO measure is based on a simple probabilistic user model. It provides monotonicity by calculating, at a given depth of evaluation, a base score that is non-decreasing with additional evaluation, and a maximum score that is nonincreasing. An extrapolated score can be calculated between these bounds if a point estimate is required. RBO has a parameter which determines the strength of the weighting to top ranks. We extend RBO to handle tied ranks and rankings of different lengths. Finally, we give examples of the use of the measure in comparing the results produced by public search engines and in assessing retrieval systems in the laboratory.},
  language = {en}
}

@article{woldPrincipalComponentAnalysis1987,
  title = {Principal Component Analysis},
  author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
  year = {1987},
  month = aug,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {2},
  number = {1-3},
  pages = {37--52},
  issn = {01697439},
  doi = {10.1016/0169-7439(87)80084-9},
  language = {en}
}

@article{wuPerformancePredictionData2006,
  title = {Performance Prediction of Data Fusion for Information Retrieval},
  author = {Wu, Shengli and McClean, Sally},
  year = {2006},
  month = jul,
  journal = {Information Processing \& Management},
  volume = {42},
  number = {4},
  pages = {899--915},
  issn = {03064573},
  doi = {10.1016/j.ipm.2005.08.004},
  language = {en}
}

@article{wuStarSpaceEmbedAll2017,
  title = {{{StarSpace}}: {{Embed All The Things}}!},
  shorttitle = {{{StarSpace}}},
  author = {Wu, Ledell and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason},
  year = {2017},
  month = nov,
  journal = {arXiv:1709.03856 [cs]},
  eprint = {1709.03856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/ramin/Zotero/storage/XAPUW7SY/Wu et al. - 2017 - StarSpace Embed All The Things!.pdf;/home/ramin/Zotero/storage/3TF2RGTT/1709.html}
}

@article{yanGraphEmbeddingExtensions2007,
  title = {Graph {{Embedding}} and {{Extensions}}: {{A General Framework}} for {{Dimensionality Reduction}}},
  shorttitle = {Graph {{Embedding}} and {{Extensions}}},
  author = {Yan, Shuicheng and Xu, Dong and Zhang, Benyu and Zhang, Hong-jiang and Yang, Qiang and Lin, Stephen},
  year = {2007},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {1},
  pages = {40--51},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2007.250598},
  file = {/home/ramin/Zotero/storage/Q8ZUTMSL/Yan et al. - 2007 - Graph Embedding and Extensions A General Framewor.pdf}
}

@inproceedings{yingGraphConvolutionalNeural2018,
  title = {Graph {{Convolutional Neural Networks}} for {{Web}}-{{Scale Recommender Systems}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
  year = {2018},
  month = jul,
  pages = {974--983},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3219890},
  isbn = {978-1-4503-5552-0},
  language = {en},
  file = {/home/ramin/Zotero/storage/B9Z6FV7P/Ying et al. - 2018 - Graph Convolutional Neural Networks for Web-Scale .pdf}
}


