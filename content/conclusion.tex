\chapter{Conclusion and Future Work}

This work introduces a partitioning middleware, which learns the partitioning of the data based on the input and output of systems and is entirely independent of the algorithm. The studied method uses an embedding model to divide text document data on various workers based on their subject. The content-based partitioning idea proposed in this work is prototyped in a recommender system called GraphJet. In the distributed environment, each worker implements the index storage, recommendation engine, and the API endpoint module of GraphJet. Each worker's recommendation results are merged to a single ranked list of items using innovative data fusion techniques described in this thesis. This research studies and measures the various test scenarios using the introduced evaluation pipeline.


It is evident that the recommendation quality will drop if a worker obtains a fraction of the data. The main goal was to generate recommendations in a multi-partition environment as identical to the single instance recommendations. The results of the experiments (with 500 sampled users in a two-partition environment) show that the proposed approach (StarSpace Most Interactions) outperforms the random partitioning method (Murmur2 Union Results). An retrival effectiveness of almost 53\% compared to 38\%. It is noticeable that the performance of StarSpace Most Interactions cannot be considered ideal with a retrival effectiveness of 53\%. On the other hand, the comparison between the single instance and the baseline achieved 63\%, denoting the not deterministic behavior of the random walk algorithm in producing recommendations.


Assuming that all the items not included in the baseline are irrelevant made the evaluation strict. Some of these items, which are not included in the baseline, might be of higher interest to the user. Moreover, using the Mean Average Precision (MAP) as the primary metric to evaluate the results does not provide enough evidence of whether the results got better or worse. The metric fails to measure if the not relevant data (the items that are not in the baseline) are of interest to the user. Therefore, no definite conclusion can be drawn as to whether the method works or not. 


In practice, recommendation data consists of user feedback, which can be gathered with a user study. The Web UI results already show more promising outcomes for a user, although further studies are needed to prove more evidence. 


Despite the limitations, these are valuable findings showing that the proposed partitioner surpasses the random partition method. The results demonstrate two things. First, the Most Interactions data fusion approach performs well with StarSpace partitioning. StarSpace partitions the data based on its subject, and when a user has a higher degree inside that partition (i.e., more interaction with documents in that partition), it is also likely that this user has more interest in the documents (i.e., tweets) from that partition. 


Second, the Union Results method relies on the hit number for choosing the recommendations. However, the hit number is heavily dependant on the amount of data on each partition. We can see that the Union Results method works better on Murmur2 and performs worse on StarSpace. This is because Murmur2 distributes the data more uniformly across the partitions compared to StarSpace.


The findings also indicate that the partitioning of the data affects the recommendation generation time. When partitioning the data on multiple workers, the data each worker holds reduces respectively. This leads to a reduction of indices on each worker. The worker maintains a sparse bipartite graph compared to the entire graph. Therefore, the random-walk algorithm can retrieve the adjacency lists of an index more quickly.


This study joins a growing corpus of research showing that latency improvements and possibly even recommendation improvements can be achieved through content-based partitioning.


Considering the above, additional research needs to be conducted to overcome the limitation and improve the performance. The following three ideas are proposed for future studies:

\begin{enumerate}
    \item Load Balancer proposed in section 3.6 provides a good starting point for discussion and further research. After implementing the load balancer, the overall performance should be evaluated. Further research can be undertaken by measuring the overhead added by the load balancer during partitioning.
    
    \item The second idea from section \ref{sec:second-usecase} presents a new use case, where the primary research approach of this work can be adopted. Future investigations are necessary to validate the concept described.
    
    \item The crawled dataset from Twitter misses feedback data on the produced recommendation. Having no feedback on the generated recommendations makes it challenging to distinguish if the yield items are relevant or not. The relevancy of an item affects the overall evaluation result. Therefore, during the assessment, this research assumes that all the results in the baseline were relevant items. This assumption limits the evaluation. Future studies can develop and study other datasets with feedback or investigate new evaluation approaches like user testing to evaluate such systems. Additionally, tools or other approaches can be used to label the dataset.Another possibility to gather feedback is through user testing. The Web UI Dashboard introduced in section 3.8 is a good starting point for further investigations and designing user testing to evaluate the recommendation quality.
\end{enumerate}