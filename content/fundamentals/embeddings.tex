\section{Embeddings}
\label{Embeddigns}
Latent text representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised way over large corpora. Work on neural embeddings in this domain includes \cite{Bengio2003}, 
\cite{Collobert2011}, word2vec \cite{goldberg2014word2vec} and more recently fastText \cite{joulin2016fasttext}.

\subsection{StarSpace}
\label{StarSpace}
StarSpace \cite{StarSpace} is a general-purpose neural model for efficient learning of entity embeddings for solving a wide variety of problems:

\begin{enumerate}
	\item Learning word, sentence, or document level embeddings.
	\item Information retrieval: ranking of sets of entities/documents or objects, e.g., ranking web documents.
	\item Metric/similarity learning, e.g., learning sentence or document similarity.
	\item Content-based or Collaborative filtering-based Recommendation, e.g., recommending music or videos.
	\item Embedding graphs, e.g., multi-relational graphs such as Freebase.
	\item Image classification, ranking, or retrieval (e.g., by using existing ResNet features).
\end{enumerate}

In my specific use case I use the so-called \emph{ArticleSpace} train mode of StarSpace. 
This particular model learns the mapping between sentences and documents.

Given the embedding of one sentence, one can find the most relevant documents. 
The model is trained as follows: Each example is an article that contains multiple sentences. At training time, one sentence is picked at random as the input, the remaining sentences in the article becomes the label, other articles are picked as random negatives.
Learning a mapping from an object to a set of objects of which it is a part, e.g. sentence (from within document) to document.
In this work, I look at StarSpace as a black box. The purpose is to use StarSpace to partition the data and not evaluate the model or different embedding tools.
