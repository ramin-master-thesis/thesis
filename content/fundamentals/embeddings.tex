\section{Embeddings}
\label{Embeddigns}
Latent text representations, or embeddings, are vectorial representations of words or documents, traditionally learned in an unsupervised way over large corpora. Work on neural embeddings in this domain includes \cite{Bengio2003}, 
\cite{Collobert2011}, word2vec \cite{goldberg2014word2vec} and more recently fastText \cite{joulin2016fasttext}.

\subsection{StarSpace}
\label{subsec:StarSpace}
StarSpace \cite{StarSpace} is a general-purpose neural model for efficient learning of entity embeddings for solving a wide variety of problems:

\begin{enumerate}
	\item Learning word, sentence, or document level embeddings.
	\item Information retrieval: ranking of sets of entities/documents or objects, e.g., ranking web documents.
	\item Metric/similarity learning, e.g., learning sentence or document similarity.
	\item Content-based or Collaborative filtering-based Recommendation, e.g., recommending music or videos.
	\item Embedding graphs, e.g., multi-relational graphs such as Freebase.
	\item Image classification, ranking, or retrieval (e.g., by using existing ResNet features).
\end{enumerate}

In this work, I use the \emph{ArticleSpace} train mode of StarSpace. This particular model learns the mapping between sentences and documents. Given the embedding of one sentence, one can find the most relevant documents. The output of the StarSpace model is a multi-dimensional vector in a so-called "Embedding-Space." Depending on the parameters that the model has used for training, the dimensionality of the output vector varies. 

For more information about the implementation of StarSpace, please refer to the code repository\footnote{\url{https://github.com/facebookresearch/StarSpace}}.


This work observes StarSpace as a black box. The purpose is to use StarSpace to partition the data and find similar content \emph{near} each other in the embedding-space, and not to improve the StarSpace model.

\subsubsection{StarSpace Text Normalization}
\label{subsubsec:star-space-text-normalization}

\subsection{Dimension Reduction}
\label{subsec:dimension-reduction}
The out dimensionaly of the embedding model is big. The dimensions can go up to 300. They are different approaches to reduce the demension and at the same time minimize the information loss. The dimension reduction in this thesis relise on a demensionality reduction method called \emph{Prenciple Componenent Analysis (PCA)} \cite{wold1987principal}.

\subsubsection{Prenciple Componenent Analysis (PCA)}
\label{subsubsec:pca}
Principal component analysis or PCA, in essence, is a linear projection operator that maps a variable of interest to a new coordinate frame where the axes represent maximal variability. Expressed mathematically, an input data matrix X$_{N \times D}$ where is N the number of points, D being the dimension of data. PCA transforms the matrix X to an output Y$_{N \times D\prime}$. D$\prime$ being the reduced demensions (D$\prime$ $\leq$ D). This reduction from D to D$\prime$ can be achived by the dot product of X and a so-called \emph{projection matrix} P$_{D \times D\prime}$.
	\begin{equation}
		\label{PCA}
		Y = XP
	\end{equation}

Each column of the projection matrix P is called a principal component (PC) \cite{projectionMatrix}. 


\subsubsection{Related work}
\label{subsubsec:dimension-reduction-related-work}
Other methods to reduce the dimension of big datasets and visualise them. \emph{t-distributed Stochastic Neighbor Embedding (t-SNE)} visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map \cite{van2008visualizing}. Unlike PCA, other methods like \emph{Linear Discriminant Analysis (LDA)} \cite{balakrishnama1998linear} are able to reduce the demension of datasets with multi classess.
