\section{Embeddings}
\label{sec:embeddings}
An \emph{embedding} of an entity (e.g., word, sentence or document) is the vector representation of that entity. An embedding should capture part of the semantics of the input by grouping semantically similar inputs in the \emph{embedding space}. The embedding space is a \emph{d}-dimensional space, where the vectors are represented. Additionally, the more dimensions the embedding space has, the more aspects of a word, sentence, or document can be represented. Figure \ref{fig:embedding-example} clarifies these explanations on a 2-dimensional embedding space. A trained embedding model calculated the vector of these documents. Each dot in this space represents these documents.
Moreover, the distance between the yellow and blue documents is smaller, indicating a semantic similarity between these two documents. In other words, the similarity between the two documents is now captured by how close these points are. On the contrary, the orange document is distant from the blue document, which means that these two documents do not represent the same topic (i.e., are not semantically similar).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{images/embedding-example}
    \caption{A simple example of document representation in a 2-dimensional embedding space. The similarity between the document is defined by how close these documents are. The yellow and blue documents are near each other, which means they are semantically similar.}
    \label{fig:embedding-example}
\end{figure}


Research on neural embeddings in this domain includes \cite{bengioNeuralProbabilisticLanguage2003}, 
\cite{collobertNaturalLanguageProcessing2011}, \emph{word2vec} \cite{mikolovEfficientEstimationWord2013}, \emph{fastText} \cite{joulinFastTextZipCompressing2016}, and \emph{BERT} \cite{devlinBERTPretrainingDeep2019}. Compared to the other researchs, StarSpace embedding model represents the document vectors by their similarites to the documents it was trained with. It also outperforms its predecessor \emph{fastText} \cite{wuStarSpaceEmbedAll2017}. In the following this embedding model is explained in more details.

\subsection{StarSpace}
\label{subsec:StarSpace}
StarSpace \cite{wuStarSpaceEmbedAll2017} is a general-purpose neural model for efficient learning of learning word, sentence, or document level embeddings.


StarSpace needs to be trained to generate the embedding vectors. This particular model learns the mapping between sentences and documents. Given the embedding of one sentence, one can find the most relevant documents. The output of the StarSpace model is a multi-dimensional vector in a so-called "Embedding-Space." Depending on the parameters that the model was trained with, the dimensionality of the output vector varies. For more information about the implementation and other use cases of StarSpace, please refer to the main repository\footnote{\url{https://github.com/facebookresearch/StarSpace}}.


This work observes StarSpace as a black box. This research aims to train StarSpace with respect to the relationship and dependency the document data has with each other. By observing the input and output of the system, these dependencies can be found. The trained StarSpace model then learns which documents are relevant and dependant on each other and can calculate the embedding vector for a newly arrived document. Afterwards, we can use these embedding vectors to classify semantically similar documents together on the same partition.

\subsubsection{StarSpace Text Normalization}
\label{subsubsec:star-space-text-normalization}
The StarSpace model uses different arguments and parameters for its training. The full documentation of these train parameter can be found in the repository\footnote{\url{https://github.com/facebookresearch/StarSpace\#full-documentation-of-parameters}}. One of the parameters that effects the input is the \emph{normalizeText} argument. This argument runs a basic text process on the input and can be either true or false. It is also one of the arguments that its impact on the partitioning is studied in this research \ref{subsec:hyperparameter-tuning}. The text normalization happens during partitioning time. For more details refer to \ref{subsec:partitioning-star-space}.


The text normalization process categorizes long strings into the following buckets:

\begin{enumerate}
    \item The whole input string consists only of punctuation-and-numeric characters, get their numbers flattened to prevent combinatorial explosions.
    \item All letters get case-flattened.
    \item Mixed letters and numbers (e.g., product ID): All the letters get case-flattened, and numbers are not changed.
\end{enumerate}

\subsection{Dimensionality Reduction}
\label{subsec:dimension-reduction}
The output vector of the embedding model has enormous dimensions. The dimensions can go up to 300. They are different approaches to reduce the dimension and at the same time minimize the information loss. The dimension reduction in this thesis relies on a dimensionality reduction method called \emph{Principal Component Analysis (PCA)} \cite{woldPrincipalComponentAnalysis1987}.

\subsubsection{Principal Component Analysis (PCA)}
\label{subsubsec:pca}
Principal component analysis (PCA), in essence, is a linear projection operator that maps a variable of interest to a new coordinate frame where the axes represent maximal variability. Expressed mathematically, an input data matrix X$_{N \times D}$ where is N the number of points, D being the dimension of data. PCA transforms the matrix X to an output Y$_{N \times D\prime}$. D$\prime$ being the reduced dimensions (D$\prime$ $\leq$ D). This reduction from D to D$\prime$ can be achieved by the dot product of X and a so-called \emph{projection matrix} P$_{D \times D\prime}$.
    \begin{equation}
        \label{PCA}
        Y = XP
    \end{equation}

\noindent Each column of the projection matrix P is called a principal component (PC)\footnote{\url{http://www.cs.cmu.edu/~tom/10601_fall2012/slides/pca.pdf}}. 


\subsubsection{Related Work}
\label{subsubsec:dimension-reduction-related-work}
Other methods exist to reduce the dimensions and visualize datasets. \emph{t-distributed Stochastic Neighbor Embedding (t-SNE)} visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map \cite{vandermaaten08}. Unlike PCA, other methods like \emph{Linear Discriminant Analysis (LDA)} \cite{martinezPCALDA2001} can reduce the dimension of datasets with multi classes. In \cite{yanGraphEmbeddingExtensions2007} study and compare dimension reduction methods like PCA, LDE, and other methods, namely Locality Preserving Projections (LPP) on graphs.
