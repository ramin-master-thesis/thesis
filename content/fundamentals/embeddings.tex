\section{Embeddings}
\label{sec:embeddings}
Latent text representations, or embeddings, are representations of words or documents as vectors, traditionally learned in an unsupervised way over large corpora. It is possible to use these embedding tools to convert the document data of this work into verctors. 

Research on neural embeddings in this domain includes \cite{bengioNeuralProbabilisticLanguage2003}, 
\cite{collobertNaturalLanguageProcessing2011}, \emph{word2vec} \cite{mikolovEfficientEstimationWord2013}, \emph{fastText} \cite{joulinFastTextZipCompressing2016}, and \emph{BERT} \cite{devlinBERTPretrainingDeep2019}. Compared to the other researchs, StarSpace embedding model represents the document vectors by their similarites to the documents it was trained with. It also outperforms its predecessor \emph{fastText} \cite{wuStarSpaceEmbedAll2017}. In the following this embedding model is explained in more details.

\subsection{StarSpace}
\label{subsec:StarSpace}
StarSpace \cite{wuStarSpaceEmbedAll2017} is a general-purpose neural model for efficient learning of learning word, sentence, or document level embeddings.


StarSpace needs to be trained to generate the embedding vectors. This particular model learns the mapping between sentences and documents. Given the embedding of one sentence, one can find the most relevant documents. The output of the StarSpace model is a multi-dimensional vector in a so-called "Embedding-Space." Depending on the parameters that the model was trained with, the dimensionality of the output vector varies. For more information about the implementation and other use cases of StarSpace, please refer to the main repository\footnote{\url{https://github.com/facebookresearch/StarSpace}}.


This work observes StarSpace as a black box. This research aims to train StarSpace with respect to the relationship and dependency the document data has with each other. By observing the input and output of the system, these dependencies can be found. The trained StarSpace model then learns which documents are relevant and dependant on each other and can calculate the embedding vector for a newly arrived document. Afterwards, we can use these embedding vectors to classify relevant documents together on the same partition.

\subsubsection{StarSpace Text Normalization}
\label{subsubsec:star-space-text-normalization}
The StarSpace model uses different arguments and parameters for its training. The full documentation of these train parameter can be found in the repository\footnote{\url{https://github.com/facebookresearch/StarSpace\#full-documentation-of-parameters}}. One of the parameters that effects the input is the \emph{normalizeText} argument. This argument runs a basic text process on the input and can be either true or false. It is also one of the arguments that its impact on the partitioning is studied in this research \ref{subsec:hyperparameter-tuning}. The text normalization happens during partitioning time. For more details refer to \ref{subsec:partitioning-star-space}.


The text normalization process categorizes long strings into the following buckets:

\begin{enumerate}
    \item The whole input string consists only of punctuation-and-numeric characters, get their numbers flattened to prevent combinatorial explosions.
    \item All letters get case-flattened.
    \item Mixed letters and numbers (e.g., product ID): All the letters get case-flattened, and numbers are not changed.
\end{enumerate}

\subsection{Dimensionality Reduction}
\label{subsec:dimension-reduction}
The output vector of the embedding model has enormous dimensions. The dimensions can go up to 300. They are different approaches to reduce the dimension and at the same time minimize the information loss. The dimension reduction in this thesis relies on a dimensionality reduction method called \emph{Prenciple Component Analysis (PCA)} \cite{woldPrincipalComponentAnalysis1987}.

\subsubsection{Prenciple Componenent Analysis (PCA)}
\label{subsubsec:pca}
Principal component analysis (PCA), in essence, is a linear projection operator that maps a variable of interest to a new coordinate frame where the axes represent maximal variability. Expressed mathematically, an input data matrix X$_{N \times D}$ where is N the number of points, D being the dimension of data. PCA transforms the matrix X to an output Y$_{N \times D\prime}$. D$\prime$ being the reduced dimensions (D$\prime$ $\leq$ D). This reduction from D to D$\prime$ can be achieved by the dot product of X and a so-called \emph{projection matrix} P$_{D \times D\prime}$.
    \begin{equation}
        \label{PCA}
        Y = XP
    \end{equation}

\noindent Each column of the projection matrix P is called a principal component (PC). 


\subsubsection{Related Work}
\label{subsubsec:dimension-reduction-related-work}
Other methods exist to reduce the dimensions and visualize datasets. \emph{t-distributed Stochastic Neighbor Embedding (t-SNE)} visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map \cite{vandermaaten08}. Unlike PCA, other methods like \emph{Linear Discriminant Analysis (LDA)} \cite{martinezPCALDA2001} can reduce the dimension of datasets with multi classes. In \cite{yanGraphEmbeddingExtensions2007} study and compare dimension reduction methods like PCA, LDE, and other methods, namely Locality Preserving Projections (LPP) on graphs.
