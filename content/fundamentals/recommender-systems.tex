\section{Recommender Systems}
\label{sec:recommender-systems}

Recommender systems use the opinions of a community of users to help individuals in that community more effectively identify the content of interest from a potentially overwhelming set of choices \cite{gunawardanaSurveyAccuracyEvaluation}.

A recommendation engine recommends a set of documents from a superset that the engine finds most relevant to the user. In that sense, a recommendation engine is simply performing the task of document retrieval. Recommender systems are mainly categorized into two types:
\begin{enumerate}
    \item \emph{Content based}
    \item \emph{Collaborative filtering (CF)}
\end{enumerate}
The recommendation engine in GraphJet \cite{sharmaGraphJetRealtimeContent2016} (explained in section \ref{subsec:GraphJet-Recommendation-Engine}) uses a collaborative filtering approach. Therefore, the main focus of this work is on collaborative filtering. The following subsection explains collaborative filtering recommender systems and related work. Lastly, the different metrics to evaluate recommender systems are discussed.

\subsection{Collaborative Filtering}
\label{subsec:collaborative-filtering}

Recommendations can be generated based on shared interests, correlated activities, topological configurations, and many other signals.

\emph{Collaborative filtering (CF)} term, which was first intorduced in \cite{goldbergUsingCollaborativeFiltering1992} means that the users in a system collaborate to perform filtering by interacting with documents and record their reactions.
In CF scenarios, the system has only access to the user and item identifiers, and no additional information over items or users is used.
Examples of recommender systems that use collaborative filtering are: Google news recommender \cite{dasGoogleNewsPersonalization2007}, Amazon recommendation system \cite{lindenAmazonComRecommendations2003}, Pixie \cite{eksombatchaiPixieSystemRecommending2018}, and GraphJet \cite{sharmaGraphJetRealtimeContent2016}.

Image a document recommender system, recommending similar documents to the users. In a typical CF scenario there is set of \emph{m} users \{\emph{$u_{1}$,$u_{2}$,...,$u_{m}$}\} and set of \emph{n} documents \{\emph{$d_{1}$,$d_{2}$,...,$d_{n}$}\}, and each user, \emph{$u_{i}$}, has a list of documents, \emph{D$u_{i}$}, which the user has interacted with. Similar to the first user there is a list of documents for user \emph{$u_{j}$}. If the intersection between the document list of these both users is not empty, these users can be considered "similar" to each other. Therefore, if user \emph{$u_{i}$} interacts with a document, \emph{$d_{i}$}, which the user \emph{D$u_{j}$} has not interacted with yet. Document \emph{$d_{i}$} can be recommended to the user \emph{$u_{j}$}. This can be seen in figure \ref{fig:collaborative-filtering}.
In \cite{suSurveyCollaborativeFiltering} the different techniques used to implement CF systems are explained in detail. Moreover, they discuss multiple evaluation metrics to assess the recommendation quality.


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.50\textwidth]{images/collaborative-filtering}
	\caption{Collaborative filtering}
	\label{fig:collaborative-filtering}
\end{figure}



\subsection{Evaluation metrics}
\label{subsec:evaluation-metrics}
One of the key challenges when developing or enhancing a recommendation engine is evaluating its impact on the quality of the recommendations. Though we are sure about the positive impact, there is a need to quantify the same.

After applying a new algorithm or an improvement to a recommendation engine, there is the need to compare two ranked lists of the main recommendation engine and the improved recommendation engine to find out the impact of the recommendation quality. The ranked list of the main recommendation engine is called \emph{objective ranking} (also called \emph{golden standard}), and after the changes are applied to the recommendation engine, the new ranked list is called \emph{observed ranking}. These two lists are assessed against each other.

% In recommender systems, the highly relevant documents are more valuable than moderately relevant documents, which are more useful than irrelevant documents.

According to \cite{webberSimilarityMeasureIndefinite2010} when comparing the golden standard with the newly generated observed ranking, the rank similarity measures can be broken down into four main groups. These measures are grouped as not-weighted (set-based) or weighted (ordered) and may need conjointness or disjoint rankings. Each of these terms is defined as follow: 

\begin{description}
    \item \emph{conjoint} Both of the ranked lists consisting of the same items.
    \item \emph{disjoint} One ranked list may include items that do not occur in the other list.
    \item \emph{weighted (ordered)} The sequence in which the items occur matters.
    \item \emph{not-weighted (set-based)} There is no known or inferred ordering of the items.
\end{description}

In this section, I explain the evaluation metrics listed in the table \ref{tab:evaluation-metrics}. For more metrics for evaluating collaborative filtering recommender system, please refer to \cite{herlockerEvaluatingCollaborativeFiltering2004}.

\begin{table}[!ht]
	\centering
	\caption{Evaluation metrics}
	\label{tab:evaluation-metrics}
	\begin{tabular}{r|l|l}
		& \textbf{Not-weighted} & \textbf{Weighted} \\
		\hline
		\textbf{Conjoint} & Kendall's $\tau$ & - \\ %Normalized Discounted Cumulative Gain (NDCG) \\
		\hline
		\textbf{Disjoint} & \vtop{\hbox{\strut AP$\atsign$K}\hbox{\strut MAP$\atsign$K}}& Ranked-Biased Overlap (RBO) \\
	\end{tabular}
\end{table}

\subsubsection{Precision and Recall}
\label{Precision and Recall}
The \emph{retrieval effectiveness}: the ability of the system to retrieve relevant documents while at the same time suppressing the retrieval of non-relevant documents. 
The most well-known pair of variables jointly measuring retrieval effectiveness is \emph{precision and recall}.

\paragraph*{Precision} is the proportion of the retrieved relevant documents. The formal definition of percision:
\begin{equation}
	percision = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{retrieved documents\}}|}
	\label{eq:percision}
\end{equation}
% Precision at k documents (P@k) is still a useful metric, but fails to take the ranking of the relevant documents into account. \cite{jarvelin2017ir}

\paragraph*{Recall} being the proportion of the relevant documents that have been retrieved. Mathematically defined as:
\begin{equation}
	recall = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{relevant documents\}}|}
	\label{eq:recall}
\end{equation}

Modern information retrieval systems nowadays retrieve thousands of relevant documents that few users will be interested in reading. Evaluating these systems with recall is obsolete.

Singly, each metric measures some aspect of retrieval effectiveness; together, they measure retrieval effectiveness entirely. Metrics like average precision bring these two metrics close to each other.

\subsubsection{AP@K and MAP@K}
\label{AP@K and MAP@K}
\paragraph*{Average precision (AP)} is the metric used to calculate retrieval effectiveness, which is defined as follows. 
Let the precision of a ranking to depth \emph{k} be
the proportion of documents to depth \emph{k} that are relevant.
Average precision for the ranking is then the sum of precisions divided by the total number of (known) relevant documents for that query.
The mathematical equation of average precision at cut-off \emph{k}:

\begin{equation}
	AP@K = \frac{1}{min(m,k)}\sum_{i=1}^{k}P(i).rel(i)
	\label{eq:ap@k}
\end{equation}

where \emph{k} is the rank in the sequence of retrieved documents, \emph{m} is the number of relevant documents, \emph{P(k)} is the precision at cut-off \emph{k} in the list. \emph{rel(k)}
is an indicator function equaling 1 if the item at rank \emph{k} is a relevant document, zero otherwise.

As an example, take a look at the tables \ref{tab:apk-example}. If we consider all the results of the objective rankings as relevant, then the AP@5 for the observed ranking is calculated as follow:

AP@5=$\frac{1}{5}$$\times$(1$\times$1 + 1$\times$$\frac{2}{2}$ + 1$\times$$\frac{3}{3}$ + 1$\times$$\frac{4}{4}$ + 0$\times$$\frac{0}{5}$) = $\frac{4}{5}$ = 0.80

\paragraph*{Mean average precision (MAP)} for a set of queries is the mean of the average precision (AP) scores for each query.

\begin{equation}
	MAP@K = \frac{1}{|U|}\sum_{u=1}^{U}(AP@K)_{u}
	\label{eq:map@k}
\end{equation}
Where \emph{U} is the number of queries.

\begin{table}[!ht]
	\caption{Comparing results of objective and observed ranking}
	\label{tab:apk-example}
	\begin{subtable}{.5\linewidth}
		\caption{Objective ranking}
		\centering
		\begin{tabular}{c|c}
			\textbf{Rank (k)}&\textbf{Recommendation} \\
			\hline
			1 & Pulp Fiction \\
			2 & Fight Club \\
			3 & Memento \\
			4 & Requiem for a Dream \\
			5 & The Shining \\
		\end{tabular}
	\end{subtable}%
	\begin{subtable}{.5\linewidth}
		\caption{Observed ranking}
		\centering
		\begin{tabular}{c|c}
			\textbf{Rank (k)}&\textbf{Recommendation} \\
			\hline
			1 & Requiem for a Dream \\
			2 & Memento \\
			3 & Fight Club \\
			4 & Pulp Fiction \\
			5 & A Separation  \\
		\end{tabular}
	\end{subtable}%
\end{table}

AP@K and MAP@K can evaluate disjoint and not-weighted ranking lists. This means the gold standard and the observed rankings contain different items, and the order of these items does not matter.

As an example of this drawback, take a look at table \ref{tab:apk-example} the AP@4 in this example is 1, although the order and ranking of the results in the observed ranking list are not the same as the objective ranking list.

% \subsubsection{CG, DCG, and NDCG}
% \label{CG, DCG, and NDCG}
% When examining the ranked result list of a query, it is obvious that:
% \begin{enumerate}
% 	\item highly relevant documents are more valuable than marginally relevant documents
% 	\item the greater the ranked position of a relevant document (of any relevance level), the less valuable it is for the user because the less likely it is that the user will examine the document.
% \end{enumerate}

% \paragraph*{Cumulative Gain (CG)}
% Every recommendation has a relevance score associated with it. Cumulative Gain is the sum of all the relevance scores in a recommendation set.

% \paragraph*{Discounted Cumulative Gain(DCG)}


% \paragraph*{Normalized Discounted Cumulative Gain (NDCG)}

\subsubsection{Kendall rank correlation coefficient}
The \emph{Kendall Tau} \cite{kendallNewMeasureRank1938} metric, also known as Kendall's Correlation, is a common method used to check if two ranked lists are in agreement. Kendall's Correlation can be computed by first counting the number of concordant pairs (C) and discordant pairs (D). A pair is said to be concordant if they appear in the same order in their ranking lists. We define M = C - D as our basic statistic. The $\tau$ is calculated with the equation below: 

\begin{equation}
    \tau = \frac{M}{(C+D)}
    \label{eq:kendall-tau}
\end{equation}

Both ranking lists need to be conjoint to calculate Kendall Tau. This metric is also not-weighted, which means that the top results matter the same as the bottom of the list.

To demonstrate these drawbacks, take a look at table \ref{tab:apk-example}. Kendall's Correlation can not be calculated since the items at rank five are different. Even by replacing the fifth element with a special character (e.g., \#) on both lists, the value of $\tau$ would be 1. This denotes perfect agreement between the two lists, which is wrong.

\subsubsection{Rank-Biased Overlap (RBO)}
The Rank-biased overlap (RBO) \cite{webberSimilarityMeasureIndefinite2010} compares two lists by looking at the \emph{Set Overlap} at each rank. The resulting overlaps are weighted by their position in the list so that differences in the top ranks are more penalized than differences at the bottom. Let \emph{S} and \emph{T} be two infinite rankings. We can define $A_{d}$ define as the agreement between \emph{S} and \emph{T} given by the proportion of the overlap up to depth \emph{d}:

\begin{equation}
	A_{d} = \frac{|S_{:d} \cap T_{:d}|}{d}
	\label{eq:a-d}
\end{equation}


RBO between the two lists \emph{S} and \emph{T} can be calculated using:

\begin{equation}
	RBO(S,T,p) = (1-p)\sum_{d=1}^{\infty}p^{d-1}.A_{d}
	\label{eq:rbo}
\end{equation}


The result is in the range of [0, 1], where zero means disjoint, and one means identical. \emph{d} is the depth of the rankings being examined, falls in the range of [1, $\infty$]. The tunable parameter \emph{p} determines how steep the decline in weights is. If \emph{p} reaches zero, the more top-weighted the metric gets. In the limit, when p = 0, only the top-ranked item is considered, and the RBO score is either zero or one. The weights get flattened when \emph{p} approaches 1.


\subsubsection{A/B testing}
\label{subsubsec:ab-testing}
\emph{A/B testing} also known as \emph{split testing} is a user experience methodology of comparing two versions of a single variable, A and B. Statical analysis is used to ascertain which variation is performing better for a conventional goal.

In works like WTF (Who To Follow) \cite{guptaWTFWhoFollow} the authors use online A/B testing to evaluate the system in production condition exclusively. This method is also used to assess Pinterest recommendation system Pixie. The authors could determine a 50\% increase of pin engagement compared to the previous recommendations systems they used before \cite{eksombatchaiPixieSystemRecommending2018}.


\subsection{Data Fusion}
Data fusion is the process of integrating and merging information gathered by multiple data sources into a single representation (i.e., ranked list) for higher effectiveness. For recommendation systems, this can be combining multiple ranked recommendation lists into a single ranked list. Data fusion approaches vary depending on the system architecture, data sources, and ranking system. A good example is MetaLens \cite{schaferMetarecommendationSystemsUsercontrolled}, a meta-recommendation system gathering information from different sources like Rotten Tomatoes and MovieLens and using them to generate more personalized recommenders.


Studies show that data fusion methods in IR systems are complex and depend highly on the system architecture and algorithm used in the system \cite{frankhsuComparingRankScore2005}. IR systems or, more specifically, recommender systems can produce a single ranked list shown in figure \ref{fig:single-machine-architecture}. The query is sent, and the system runs the algorithm and outputs the ranked list to the user. The query is sent and can get processed and normalized using a query \emph{formulation}. Afterward, a \emph{schema} processes the query formulation and generates a single ranked list. In \ref{fig:single-machine-architecture} the query is a simple user-ID and does not need to go through the formulation. 


With the recent developments in computer science and distributed systems, it is possible to distribute recommender systems into multiple instances. \cite{frankhsuComparingRankScore2005} proposes three main architectures for multi-machine IR systems:
\begin{enumerate}
    \item Multiple formulations single scheme (MFSS)
    \item Single formulation multiple schemes (SFMS)
    \item Multiple formulations multiple schemes (MFMS)
\end{enumerate}
This work uses an SFMS architecture for the distribution. In this architecture, the query is sent to a single formulation point and from there to multiple schemes (i.e., workers). Each scheme runs the recommendation algorithm and produces a ranked list. In the last step, these ranked lists are then integrated into a single ranked list (data fusion approaches explained in section \ref{sec:data-fusion-approaches}). Figure \ref{fig:multiple-machine-architecture} indicates such design.

