\section{Recommender Systems}
\label{Grelgdnuan_vno_Bleototuh_Lwo_Egerny_BLE}



\subsection{Metrics}
\label{IR Metrics}
One of the key challenges when developing or enhancing a recommendation engine is evaluating its impact. Though we are sure about the positive impact, there is a need to quantify the same. This quantification not only facilitates stakeholder communication but also serves as a benchmark for future enhancements.

A recommendation engine recommends a set of documents from a superset which the engine finds to be most relevant to the user. In that sense, a recommendation engine is simply performing a task of document retrieval.

The highly relevant documents are more valuable than moderately relevant documents, which are in turn more useful than irrelevant documents.

In IR, as in many other domains, it can be important to compare the similarity, or consistency, of groups of things. These groups may be:

\begin{description}
	\item \emph{conjoint (consisting of the same items)} or \emph{disjoint (one group may include items that do not occur in the other group)}
	\item \emph{set-based (not-weighted) (where there is no known or inferred ordering of the items)} or \emph{ordered (weighted) (where the sequence in which the items occur matters)}
\end{description}

In our use case (SALSA), the lists are disjoint and ordered.
\begin{enumerate}
	\item AP@K and MAP@K: Not-Weighted conjoint
	\item NDCG: Weighted non-conjoint measures
	\item Kendall: Not-Weighted conjoint
	\item RBO: Weighted non-conjoint measures
\end{enumerate}

\subsubsection{Precision and Recall}
\label{Precision and Recall}
The \emph{retrieval effectiveness}: the ability of the system to retrieve relevant documents while at the same time suppressing the retrieval of non-relevant documents. 
The most well-known pair of variables jointly measuring retrieval effectiveness are \emph{precision and recall}.

\emph{Precision} is the proportion of the retrieved relevant documents.
\begin{equation}
	percision = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{retrieved documents\}}|}
	\label{eq:percision}
\end{equation}
Precision at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results among the top 10 retrieved documents), 
but fails to take into account the positions of the relevant documents among the top k. [3]

\emph{Recall} being the proportion of the relevant documents that have been retrieved.
\begin{equation}
	recall = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{relevant documents\}}|}
	\label{eq:recall}
\end{equation}

For modern (web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them.

Singly, each variable (or parameter as it is sometimes called) measures some aspect of retrieval effectiveness; jointly they measure retrieval effectiveness completely. [2]

\subsubsection{AP@K and MAP@K}
\label{AP@K and MAP@K}
Retrieval effectiveness evaluation metrics; score a document ranking according to the relevance of the documents it contains. 
\paragraph*{Average percision (AP)} is the metric used to calculate retrieval effectiveness, which is defined as follows. 
Let the precision of a ranking to depth \emph{k} be
the proportion of documents to depth \emph{k} that are relevant. The sum of precisions for that
ranking is the sum of the precision at each ranking that a relevant document is returned.
Average precision for the ranking is then the sum of precisions divided by the total number of (known) relevant documents for that query. [1] 
The mathematical equation of AP@K is defined as follow.

\begin{equation}
	AP@K = \frac{1}{m}\sum_{i=1}^{k}P(i).rel(i)
	\label{eq:ap@k}
\end{equation}

where \emph{k} is the rank in the sequence of retrieved documents, \emph{n} is the number of retrieved documents, \emph{P(k)} is the precision at cut-off \emph{k} in the list. \emph{rel(k)}
is an indicator function equaling 1 if the item at rank \emph{k} is a relevant document, zero otherwise.

\paragraph*{Mean average precision (MAP)} for a set of queries is the mean of the average precision (AP) scores for each query.

\begin{equation}
	MAP@K = \frac{1}{|U|}\sum_{u=1}^{U}(AP@K)_{u}
	\label{eq:map@k}
\end{equation}
Where \emph{U} is the number of queries.

\paragraph*{Drawbacks}
If all the items retrivied form an IR sytem are cosidered as relevant the order of the items does not effect the average percision.
 The firs example demonstrates this problem.
actual = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
predicted = [10, 6, 9, 7, 8, 3, 2, 5, 4, 1]
AP = 1
------------------------------------------
actual = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
predicted = [1, 20,30,40,50,60,70,80,90,100]
AP = 1/10

\subsubsection{CG, DCG, and NDCG}
\label{CG, DCG, and NDCG}
When examining the ranked result list of a query, it is obvious that:
\begin{enumerate}
	\item highly relevant documents are more valuable than marginally relevant documents
	\item the greater the ranked position of a relevant document (of any relevance level) the less valuable it is for the user because the less likely it is that the user will examine the document.
\end{enumerate}

\paragraph*{Cumulative Gain (CG)}
Every recommendation has a relevance score associated with it. Cumulative Gain is the sum of all the relevance scores in a recommendation set.

\paragraph*{Discounted Cumulative Gain(DCG)}


\paragraph*{Normalized Discounted Cumulative Gain (NDCG)}

\subsubsection{Kendall rank correlation coefficient}
The Kendall Tau metric also known as Kendall’s Correlation is a common method used to check if two ranked lists are in agreement.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.50\textwidth]{images/metrics/kendall-example}
	\caption{The Kendall Tau metric also known as Kendall’s Correlation is a common method used to check if two ranked lists are in agreement.}
	\label{fig:kendall-rank-example}
\end{figure}

\subsubsection{Rank-Biased Overlap (RBO)}
Rank-biased overlap falls in the range [0, 1], where 0 means disjoint, and 1 means identical. The parameter p determines how steep the decline in weights is: the smaller p, the
more top-weighted the metric is. In the limit, when p = 0, only the top-ranked item is considered, and the RBO score is either zero or one. 
On the other hand, as p approaches arbitrarily close to 1, the weights become arbitrarily flat, and the evaluation becomes arbitrarily deep.

\begin{equation}
	RBO(S,T,p) = (1-p)\sum_{d=1}^{\infty}p^{d-1}.A_{d}
	\label{eq:rbo}
\end{equation}
