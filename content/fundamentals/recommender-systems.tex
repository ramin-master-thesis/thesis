\section{Recommender Systems}
\label{sec:recommender-systems}

Recommender systems use the opinions of a community of users to help individuals in that community more effectively identify the content of interest from a potentially overwhelming set of choices \cite{resnick1997recommender}.

A recommendation engine recommends a set of documents from a superset which the engine finds to be most relevant to the user. In that sense, a recommendation engine is simply performing a task of document retrieval. Recommender systems can be broadly categorized into two types:
\begin{enumerate}
    \item \emph{Content based}
    \item \emph{Collaborative filtering (CF)}
\end{enumerate}
The recommendation engine in GraphJet \cite{sharma2016graphjet} (explained in section \ref{subsec:GraphJet-Recommendation-Engine}) uses a collaborative filtering approach. Therefore, the main focus is on collaborative filtering. The following subsection explains collaborative filtering recommender systems and related work. Afterward, the different metrics to evaluate recommender systems are discussed.

\subsection{Collaborative Filtering}
\label{subsec:collaborative-filtering}

Recommendations can be generated based on shared interests, correlated activities, topological configurations, and many other signals.

\emph{Collaborative filtering (CF)} term, which was first announced in \cite{goldberg1992using} means that the users in a system collaborate to perform filtering by interacting with documents and record their reactions.
In CF scenarios, the system only has access to the user and item identifiers, and no additional information over items or users is used.
Examples of recommender systems that use collaborative filtering are: Google news recommender \cite{das2007google}, Amazon recommendation system \cite{linden2003amazon}, Pixie \cite{eksombatchai2018pixie}, GraphJet \cite{sharma2016graphjet}.

Image a document recommender system, recommending similar documents to the users. In a typical CF scenario there is list of \emph{m} users \{\emph{$u_{1}$,$u_{2}$,...,$u_{m}$}\} and list of \emph{n} documents \{\emph{$d_{1}$,$d_{2}$,...,$d_{n}$}\}, and each user, \emph{$u_{i}$}, has a list of documents, \emph{D$u_{i}$}, which the user has interacted with. Similar to the first user there is a list of documents for user \emph{$u_{j}$}. If the intersection between the document list of these both users is not empty, these users can be considered "similar" to each other. Therefore, if user \emph{$u_{i}$} interacts with a document, \emph{D$d_{i}$}, which the user \emph{D$u_{j}$} has not interacted with yet, this can be recommended to the \emph{D$u_{j}$}. This can be seen in figure \ref{fig:collaborative-filtering}.
In \cite{su2009survey} the different techniques used to implement CF systems are explained in detail. Moreover, they discuss multiple evaluation metrics to assess the recommendation quality.


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.50\textwidth]{images/collaborative-filtering}
	\caption{Collaborative filtering}
	\label{fig:collaborative-filtering}
\end{figure}



\subsection{Evaluation metrics}
\label{subsec:evaluation-metrics}
One of the key challenges when developing or enhancing a recommendation engine is evaluating its impact on the quality of the recommendations. Though we are sure about the positive impact, there is a need to quantify the same.

After applying a new algorithm or an improvement to a recommendation engine, there is the need to compare two ranked lists of the main recommendation engine and the improved recommendation engine to find out the impact of the recommendation quality. The ranked list of the main recommendation engine is called \emph{objective ranking} (also called \emph{golden standard}), and after the changes are applied to the recommendation engine, the new ranked list is called \emph{observed ranking}. These two lists are assessed against each other.

% In recommender systems, the highly relevant documents are more valuable than moderately relevant documents, which are more useful than irrelevant documents.

According to \cite{webber2010similarity} when comparing the golden standard with the newly generated observed ranking, the rank similarity measures can be broken down into four main groups. These measures are grouped as not-weighted (set-based) or weighted (ordered) and may need conjointness or disjoint rankings. Each of these terms is defined as follow: 

\begin{description}
    \item \emph{conjoint} Both of the ranked lists consisting of the same items.
    \item \emph{disjoint} One ranked list may include items that do not occur in the other list.
    \item \emph{weighted (ordered)} The sequence in which the items occur matters.
    \item \emph{not-weighted (set-based)} There is no known or inferred ordering of the items.
\end{description}

In this section I explain the evaluation metrics listed in the table \ref{tab:evaluation-metrics}. For more metrics for evaluating collaborative filtering recommender system please refere to \cite{herlocker2004evaluating}.

\begin{table}[!ht]
	\centering
	\caption{Evaluation metrics}
	\label{tab:evaluation-metrics}
	\begin{tabular}{r|l|l}
		& \textbf{Not-weighted} & \textbf{Weighted} \\
		\hline
		\textbf{Conjoint} & \vtop{\hbox{\strut AP$\atsign$K}\hbox{\strut MAP$\atsign$K}\hbox{\strut Kendall's $\tau$}} & Normalized Discounted Cumulative Gain (NDCG) \\
		\hline
		\textbf{Disjoint} & - & Ranked-Biased Overlap (RBO) \\
	\end{tabular}
\end{table}

\subsubsection{Precision and Recall}
\label{Precision and Recall}
The \emph{retrieval effectiveness}: the ability of the system to retrieve relevant documents while at the same time suppressing the retrieval of non-relevant documents. 
The most well-known pair of variables jointly measuring retrieval effectiveness is \emph{precision and recall}.

\paragraph*{Precision} is the proportion of the retrieved relevant documents. The formal definition of percision:
\begin{equation}
	percision = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{retrieved documents\}}|}
	\label{eq:percision}
\end{equation}
% Precision at k documents (P@k) is still a useful metric, but fails to take the ranking of the relevant documents into account. \cite{jarvelin2017ir}

\paragraph*{Recall} being the proportion of the relevant documents that have been retrieved. Mathematically defined as:
\begin{equation}
	recall = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{relevant documents\}}|}
	\label{eq:recall}
\end{equation}

Moder information retrieval systems now a days retirvie thousands of relevant documents that few users will be interested in reading all of them; evaluating these systems with recall is obsolet 

Singly, each metric measures some aspect of retrieval effectiveness; together, they measure retrieval effectiveness completely. To bring these two metrics close to each other metrics like average percision are introduced.

\subsubsection{AP@K and MAP@K}
\label{AP@K and MAP@K}
\paragraph*{Average precision (AP)} is the metric used to calculate retrieval effectiveness, which is defined as follows. 
Let the precision of a ranking to depth \emph{k} be
the proportion of documents to depth \emph{k} that are relevant. The sum of precisions for that ranking is the sum of the precision at each ranking that a relevant document is returned.
Average precision for the ranking is then the sum of precisions divided by the total number of (known) relevant documents for that query.
The mathematical equation of average percision at k:

\begin{equation}
	AP@K = \frac{1}{m}\sum_{i=1}^{k}P(i).rel(i)
	\label{eq:ap@k}
\end{equation}

where \emph{k} is the rank in the sequence of retrieved documents, \emph{n} is the number of retrieved documents, \emph{P(k)} is the precision at cut-off \emph{k} in the list. \emph{rel(k)}
is an indicator function equaling 1 if the item at rank \emph{k} is a relevant document, zero otherwise.

\paragraph*{Mean average precision (MAP)} for a set of queries is the mean of the average precision (AP) scores for each query.

\begin{equation}
	MAP@K = \frac{1}{|U|}\sum_{u=1}^{U}(AP@K)_{u}
	\label{eq:map@k}
\end{equation}
Where \emph{U} is the number of queries.

\todo{Put a figure here and make an example how to calculate AP@K and MAP@K}

As explained before AP@K and MAP@K are considered as conjoint and not-weighted metrics. This means the gold standard and the observed rankings should contain the same items but the order of these items does not matter if all of them already exists in the observed ranking. Table \ref{tab:apk-drawback} shows this drawback.

\todo{Create a figure to explain the drawback of AP@K}
The first example demonstrates this problem.
actual = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
predicted = [10, 6, 9, 7, 8, 3, 2, 5, 4, 1]
AP = 1


\subsubsection{CG, DCG, and NDCG}
\label{CG, DCG, and NDCG}
When examining the ranked result list of a query, it is obvious that:
\begin{enumerate}
	\item highly relevant documents are more valuable than marginally relevant documents
	\item the greater the ranked position of a relevant document (of any relevance level) the less valuable it is for the user because the less likely it is that the user will examine the document.
\end{enumerate}

\paragraph*{Cumulative Gain (CG)}
Every recommendation has a relevance score associated with it. Cumulative Gain is the sum of all the relevance scores in a recommendation set.

\paragraph*{Discounted Cumulative Gain(DCG)}


\paragraph*{Normalized Discounted Cumulative Gain (NDCG)}

\subsubsection{Kendall rank correlation coefficient}
The Kendall Tau metric also known as Kendall's Correlation is a common method used to check if two ranked lists are in agreement.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.50\textwidth]{images/metrics/kendall-example}
	\caption{The Kendall Tau metric also known as Kendallâ€™s Correlation is a common method used to check if two ranked lists are in agreement.}
	\label{fig:kendall-rank-example}
\end{figure}

\subsubsection{Rank-Biased Overlap (RBO)}
Rank-biased overlap \cite{webber2010similarity} falls in the range [0, 1], where 0 means disjoint, and 1 means identical. The parameter p determines how steep the decline in weights is: the smaller p, the
more top-weighted the metric is. In the limit, when p = 0, only the top-ranked item is considered, and the RBO score is either zero or one. 
On the other hand, as p approaches arbitrarily close to 1, the weights become arbitrarily flat, and the evaluation becomes arbitrarily deep.

\begin{equation}
	RBO(S,T,p) = (1-p)\sum_{d=1}^{\infty}p^{d-1}.A_{d}
	\label{eq:rbo}
\end{equation}
