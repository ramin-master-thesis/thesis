\section{Recommender Systems}
\label{sec:recommender-systems}

Recommender systems use the opinions of a community of users to help individuals in that community more effectively identify the content of interest from a potentially overwhelming set of choices \cite{resnick1997recommender}.

A recommendation engine recommends a set of documents from a superset which the engine finds to be most relevant to the user. In that sense, a recommendation engine is simply performing a task of document retrieval. Recommender systems can be broadly categorized into two types:
\begin{enumerate}
    \item \emph{Content based}
    \item \emph{Collaborative filtering (CF)}
\end{enumerate}
The recommendation engine in GraphJet \cite{sharma2016graphjet} (explained in section \ref{subsec:GraphJet-Recommendation-Engine}) uses a collaborative filtering approach. Therefore, the main focus is on collaborative filtering. The following subsection explains collaborative filtering recommender systems and related work. Afterward, the different metrics to evaluate recommender systems are discussed.

\subsection{Collaborative Filtering}
\label{subsec:collaborative-filtering}

Recommendations can be generated based on shared interests, correlated activities, topological configurations, and many other signals.

\emph{Collaborative filtering (CF)} term, which was first announced in \cite{goldberg1992using} means that the users in a system collaborate to perform filtering by interacting with documents and record their reactions.
In CF scenarios, the system only has access to the user and item identifiers, and no additional information over items or users is used.
Examples of recommender systems that use collaborative filtering are: Google news recommender \cite{das2007google}, Amazon recommendation system \cite{linden2003amazon}, Pixie \cite{eksombatchai2018pixie}, GraphJet \cite{sharma2016graphjet}.

Image a document recommender system, recommending similar documents to the users. In a typical CF scenario there is list of \emph{m} users \{\emph{$u_{1}$,$u_{2}$,...,$u_{m}$}\} and list of \emph{n} documents \{\emph{$d_{1}$,$d_{2}$,...,$d_{n}$}\}, and each user, \emph{$u_{i}$}, has a list of documents, \emph{D$u_{i}$}, which the user has interacted with. Similar to the first user there is a list of documents for user \emph{$u_{j}$}. If the intersection between the document list of these both users is not empty, these users can be considered "similar" to each other. Therefore, if user \emph{$u_{i}$} interacts with a document, \emph{D$d_{i}$}, which the user \emph{D$u_{j}$} has not interacted with yet, this can be recommended to the \emph{D$u_{j}$}. This can be seen in figure \ref{fig:collaborative-filtering}.
In \cite{su2009survey} the different techniques used to implement CF systems are explained in detail. Moreover, they discuss multiple evaluation metrics to assess the recommendation quality.


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.50\textwidth]{images/collaborative-filtering}
	\caption{Collaborative filtering}
	\label{fig:collaborative-filtering}
\end{figure}



\subsection{Evaluation metrics}
\label{subsec:evaluation-metrics}
One of the key challenges when developing or enhancing a recommendation engine is evaluating its impact on the quality of the recommendations. Though we are sure about the positive impact, there is a need to quantify the same.

After applying a new algorithm or an improvement to a recommendation engine, there is the need to compare two ranked lists of the main recommendation engine and the improved recommendation engine to find out the impact of the recommendation quality. The ranked list of the main recommendation engine is called \emph{objective ranking} (also called \emph{golden standard}), and after the changes are applied to the recommendation engine, the new ranked list is called \emph{observed ranking}. These two lists are assessed against each other.

% In recommender systems, the highly relevant documents are more valuable than moderately relevant documents, which are more useful than irrelevant documents.

According to \cite{webber2010similarity} when comparing the golden standard with the newly generated observed ranking, the rank similarity measures can be broken down into four main groups. These measures are grouped as not-weighted (set-based) or weighted (ordered) and may need conjointness or disjoint rankings. Each of these terms is defined as follow: 

\begin{description}
    \item \emph{conjoint} Both of the ranked lists consisting of the same items.
    \item \emph{disjoint} One ranked list may include items that do not occur in the other list.
    \item \emph{weighted (ordered)} The sequence in which the items occur matters.
    \item \emph{not-weighted (set-based)} There is no known or inferred ordering of the items.
\end{description}

In this section I explain the evaluation metrics listed in the table \ref{tab:evaluation-metrics}. For more metrics for evaluating collaborative filtering recommender system please refere to \cite{herlocker2004evaluating}.

\begin{table}[!ht]
	\centering
	\caption{Evaluation metrics}
	\label{tab:evaluation-metrics}
	\begin{tabular}{r|l|l}
		& \textbf{Not-weighted} & \textbf{Weighted} \\
		\hline
		\textbf{Conjoint} & Kendall's $\tau$ & Normalized Discounted Cumulative Gain (NDCG) \\
		\hline
		\textbf{Disjoint} & \vtop{\hbox{\strut AP$\atsign$K}\hbox{\strut MAP$\atsign$K}}& Ranked-Biased Overlap (RBO) \\
	\end{tabular}
\end{table}

\subsubsection{Precision and Recall}
\label{Precision and Recall}
The \emph{retrieval effectiveness}: the ability of the system to retrieve relevant documents while at the same time suppressing the retrieval of non-relevant documents. 
The most well-known pair of variables jointly measuring retrieval effectiveness is \emph{precision and recall}.

\paragraph*{Precision} is the proportion of the retrieved relevant documents. The formal definition of percision:
\begin{equation}
	percision = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{retrieved documents\}}|}
	\label{eq:percision}
\end{equation}
% Precision at k documents (P@k) is still a useful metric, but fails to take the ranking of the relevant documents into account. \cite{jarvelin2017ir}

\paragraph*{Recall} being the proportion of the relevant documents that have been retrieved. Mathematically defined as:
\begin{equation}
	recall = \frac{|\emph{\{relevant document\}} \cap \emph{\{retrieved documents\}}|}{|\emph{\{relevant documents\}}|}
	\label{eq:recall}
\end{equation}

Moder information retrieval systems nowadays retrieve thousands of relevant documents that few users will be interested in reading all of them; evaluating these systems with recall is obsolete.

Singly, each metric measures some aspect of retrieval effectiveness; together, they measure retrieval effectiveness entirely. To bring these two metrics close to each other, metrics like average precision are introduced.

\subsubsection{AP@K and MAP@K}
\label{AP@K and MAP@K}
\paragraph*{Average precision (AP)} is the metric used to calculate retrieval effectiveness, which is defined as follows. 
Let the precision of a ranking to depth \emph{k} be
the proportion of documents to depth \emph{k} that are relevant.
Average precision for the ranking is then the sum of precisions divided by the total number of (known) relevant documents for that query.
The mathematical equation of average percision at k:

\begin{equation}
	AP@K = \frac{1}{min(m,k)}\sum_{i=1}^{k}P(i).rel(i)
	\label{eq:ap@k}
\end{equation}

where \emph{k} is the rank in the sequence of retrieved documents, \emph{m} is the number of relevant documents, \emph{P(k)} is the precision at cut-off \emph{k} in the list. \emph{rel(k)}
is an indicator function equaling 1 if the item at rank \emph{k} is a relevant document, zero otherwise.

As an example take a look at the tables \ref{tab:apk-example}. If we consider all the results fo the objective rankings as relevant then the AP@5 for the observed ranking is calculate as follow:

AP@5=$\frac{1}{5}$$\times$(1$\times$1 + 1$\times$$\frac{2}{2}$ + 1$\times$$\frac{3}{3}$ + 1$\times$$\frac{4}{4}$ + 0$\times$$\frac{0}{5}$) = $\frac{4}{5}$ = 0.80

\paragraph*{Mean average precision (MAP)} for a set of queries is the mean of the average precision (AP) scores for each query.

\begin{equation}
	MAP@K = \frac{1}{|U|}\sum_{u=1}^{U}(AP@K)_{u}
	\label{eq:map@k}
\end{equation}
Where \emph{U} is the number of queries.

\begin{table}[!ht]
	\caption{Comparing results of objective and observed ranking}
	\label{tab:apk-example}
	\begin{subtable}{.5\linewidth}
		\caption{Objective ranking}
		\centering
		\begin{tabular}{c|c}
			\textbf{Rank (k)}&\textbf{Recommendation} \\
			\hline
			1 & Pulp Fiction \\
			2 & Fight Club \\
			3 & Memento \\
			4 & Requiem for a Dream \\
			5 & The Shining \\
		\end{tabular}
	\end{subtable}%
	\begin{subtable}{.5\linewidth}
		\caption{Observed ranking}
		\centering
		\begin{tabular}{c|c}
			\textbf{Rank (k)}&\textbf{Recommendation} \\
			\hline
			1 & Requiem for a Dream \\
			2 & Memento \\
			3 & Fight Club \\
			4 & Pulp Fiction \\
			5 & A Separation  \\
		\end{tabular}
	\end{subtable}%
\end{table}

As explained before, AP@K and MAP@K are considered as disjoint and not-weighted metrics. This means the gold standard and the observed rankings contain the different items. Moreover, the order of these items does not matter if all of them already exists in the observed ranking.

As an example of this drawback take a look at table \ref{tab:apk-example} the AP@4 in this example is 1, although the order and ranking of the results in the observed ranking list is not the same as the objective ranking list.

% \subsubsection{CG, DCG, and NDCG}
% \label{CG, DCG, and NDCG}
% When examining the ranked result list of a query, it is obvious that:
% \begin{enumerate}
% 	\item highly relevant documents are more valuable than marginally relevant documents
% 	\item the greater the ranked position of a relevant document (of any relevance level) the less valuable it is for the user because the less likely it is that the user will examine the document.
% \end{enumerate}

% \paragraph*{Cumulative Gain (CG)}
% Every recommendation has a relevance score associated with it. Cumulative Gain is the sum of all the relevance scores in a recommendation set.

% \paragraph*{Discounted Cumulative Gain(DCG)}


% \paragraph*{Normalized Discounted Cumulative Gain (NDCG)}

\subsubsection{Kendall rank correlation coefficient}
The \emph{Kendall Tau} \cite{kendall1938new} metric, also known as Kendall's Correlation, is a common method used to check if two ranked lists are in agreement. Kendall's correlation can be computed by first counting the number of concordant pairs (C) and the number of discordant pairs (D). A pair is said to be concordant if they appear in the same order in their ranking lists. Them M = C - D is our basic statistic. The $\tau$ is calculated: 

\begin{equation}
	\tau = \frac{M}{(C+D)}
	\label{eq:kendall-tau}
\end{equation}

To calculate Kendall Tau both ranking lists needs to be conjoint. This metric is also not-weighted, this means that the top results matter the same as the bottom of the list.

To demonstrait these drawbacks take a look at table \ref{tab:apk-example}. Kendall's correlation can not be calculated since the items at rank 5 are different. Even by replacing the fifth elemnt with special character (e.g. \#) on both lists, the value of $\tau$ would be 1. This denotes perfect agreement between the two lists, which is not true.

\subsubsection{Rank-Biased Overlap (RBO)}
The Rank-biased overlap \cite{webber2010similarity} compares two lists by looking at the Set Overlap at each rank. The resulting overlaps are weighted by their position in the list so that differences in the top ranks are more penalized than differences at the bottom. Let \emph{S} and \emph{T} be two infinite rankings. We can define $A_{d}$ define as the agreement between \emph{S} and \emph{T} given by the proportion of the overlap up to depth \emph{d}:

\begin{equation}
	A_{d} = \frac{|S_{:d} \cap T_{:d}|}{d}
	\label{eq:a-d}
\end{equation}


RBO between the two lists \emph{S} and \emph{T} can be calculated using:

\begin{equation}
	RBO(S,T,p) = (1-p)\sum_{d=1}^{\infty}p^{d-1}.A_{d}
	\label{eq:rbo}
\end{equation}


The result is in the range of [0, 1], where 0 means disjoint, and 1 means identical. \emph{d} is the depth of the rankings being examined, falls in the range of [1, $\infty$]. The tuneable parameter \emph{p} determines how steep the decline in weights is. If \emph{p} converges to zero, the more top-weighted the metric gets. In the limit, when p = 0, only the top-ranked item is considered, and the RBO score is either zero or one. The weights get flatterned when \emph{p} approaches 1.


\subsection{Data Fusion}
Data fusion is the process of integrating information gathered by multiple data sources into a single representation \cite{hsu2005comparing}. For recommendation systems, this can be combining multiple ranked recommendation lists into a single ranked list. Data fusion approaches vary depending on the system architecture, data sources, and ranking system. A good example is MetaLens \cite{schafer2002meta}, a meta-recommendation system gathering information from different sources like Rotten Tomatoes and MovieLens and using them to generate more personalized recommenders.
 \cite{wu2006performance}

