\section{Latency}
\label{sec:eval-latency}
The main focus of this work's evaluation is how partitioning affects the recommendation quality. But during the assessment of the system, I noticed that the computation time of recommendation in smaller partitions sinks notably. Therefore, it was worth investigating, measuring, and discuss the latency improvements.


The focus of the evaluation here is to measure the time of each instance computing the recommendations using the SALSA algorithm. The intuition here is that by partitioning the data on multiple machines, the amount of data on each machine reduces respectively, leading to a faster random walk on the bipartite graph. 

\subsection{Hardware specification}
\label{subsec:hardware-spec}
This section describes the hardware specification, where the expriments and evaluation of the proposed thesis ran.

The instance is a VM-Host-Server with 2x AMD EPYC 7282 (Zen-Rome) 16-Core CPU. The CPU clock speed is 2.80GHz and can reach up to 3.20GHz. The machine has 256GB (16x 16GB) DDR4-3200 RAM. The storage is a Samsung PM1725b SSD 1.6TB. The virtualization is runing on QEMU-KVM/libvirt version 4.2.1 (Debian 1:4.2-3ubuntu6.17).


\subsection{Experiment strategy}
\label{subsec:latency-experiment-strategy}
The benchmarks are done over one, two, and four partitions. The partitioning method used to distribute the test dataset is \emph{Murmur2 Hash} (see section \ref{subsec:partitioning-murmur2}). Using only this partitioning method is sufficient for the latency evaluation since we can see the relation between the amount of data on each partition and the execution time of the random walk and the data fusion approaches.

The experiments are repeated ten times. On each run, 500 new users are sampled, and the recommendations are computed; then, the three data fusion approaches are used to fusion the results.


It is crucial to notice that the code used to simulate GraphJet is not a production code. Thus, these results can also be excepted in a production-like system. The simulated GraphJet system is written in Python (3.8) and uses Pandas' (version 1.2.3) DataFrame to store and retrieve the left, right-side indices. Furthermore, Python's Flask (version 1.1.2) is used to send the request to each partition (i.e., instance).


\subsection{Recommendation Computation Latency}
\label{subsec:recommendation-computation-latency}
This section discusses how the amount of data impacts the random walk computation time. The experiments assess the recommendation generation latency of a single instance with the multi-partitioned instance.


The experiments investigate the random walk latency in a scenario with two partitions and once in an four partition enviroment. The diagrams \ref{plot:edge-distribution-2-partitions-murmur2} and \ref{plot:edge-distribution-4-partitions-murmur2} show the data distribution with respect to the mentioned partitioning strategy. At first glance, the diagrams denote that partitioning the data with the Murmur2 hash method distributes the edges almost equally. 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \input{plots/latency/murmur2-edge-distribution-2-partitions.tex}
        \caption{2 partitions}
        \label{plot:edge-distribution-2-partitions-murmur2}
    \end{subfigure}\qquad

    \begin{subfigure}{\textwidth}
        \centering
        \input{plots/latency/murmur2-edge-distribution-4-partitions.tex}
        \caption{4 partitions}
        \label{plot:edge-distribution-4-partitions-murmur2}
    \end{subfigure}\qquad
    \caption{Murmur2 hash function edge distribution.}
    \label{plot:edge-distirbution-murmur2}
\end{figure}


The results of the latency experiments are demonstrated in the diagrams \ref{plot:recommendation-latency}. The boxplot in \ref{plot:recommendation-latency-two-partitions} indicates the difference between the single partition and two partition instances. There is approximately 9.5\% latency improvement when comparing the single partition latency with the first partition (i.e., partition 0). The latency improves when the amount of partitions increases. The plot in \ref{plot:recommendation-latency-two-partitions} demonstrates this. The latency improves over 40\% with the increase of partitions. 

This improvement can be explained when we look at the data distribution on each partition in figure \ref{plot:edge-distirbution-murmur2}. Each partition holds a fraction of the data, making it possible for each instance to maintain a smaller number of indices and adjacency lists. Therefore, the random walk algorithm can faster retrieve an index with its adjacency list. Concluding, by increasing the number of partitions, the amount of data on each instance reduces, and the recommendation computation time improves exponentially.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \input{plots/latency/recommendation-latency-two-partition-boxplot}
        \caption{Single partition recommendation generation latency compared to two partitions}
        \label{plot:recommendation-latency-two-partitions}
    \end{subfigure}\qquad

    \begin{subfigure}{\textwidth}
        \centering
        \input{plots/latency/recommendation-latency-four-partition-boxplot}
        \caption{Recommendation generation latency in four partition}
        \label{plot:recommendation-latency-four-partitions}
    \end{subfigure}\qquad
    \caption{Recommendation generation latency in seconds.}
    \label{plot:recommendation-latency}
\end{figure}



\subsection{Data Fusion Latency}
\label{subsec:data-fusion-latency}
The multi-partition environment uses the data fusion module to generate a single ranked list. Therefore, this module should also be included in latency benchmarks. The experiment results in table \ref{tab:data-fusion-latency} show that the data fusion approaches are not a bottleneck and deliver a "good enough" latency.


The \emph{Most Interactions} method has the most significant latency. For each user, the data fusion module needs to make a network call to a partition to retrieve the degree of the user, making it slower compared to the other data fusion approaches. If the amount of partitions doubles, the latency of this particular approach almost doubles respectively.


The idea of storing the degrees of the user during partitioning time and using a load balancer to make the correct partition mentioned in \ref{fig:loadbalancer} could improve the latency. Although, further investigations are needed to benchmark the latency of the proposed load balancer idea.


\begin{table}[!ht]
    \centering
    \caption{Average latency in seconds of different data fusion approaches for 500 users}
    \label{tab:data-fusion-latency}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Union Results} & \textbf{Highest Hit} & \textbf{Most Interactions} \\
        \hline
        0.13 & 0.50 & 4.55 \\
        \hline
    \end{tabular}
\end{table}
