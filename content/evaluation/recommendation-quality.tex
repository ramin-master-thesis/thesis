\section{Recommendation Quality}
\label{sec:recommendation-quality}
This section will go through the expriments to evaluate the impact of the partitioning over the recommendations. The first part of this section, determines the expriment setup and different parameters values used to build the scenarios. The next sections, describe and discuss the results in different scenarios.


\subsection{Experiment setup}
\label{subsec:experiment-setup}
As explained in section \ref{sec:evaluation-pipeline} they are parameters that we can inject in the pipeline that influences the pipeline. Choosing different partitioning methods, amount of partitions, data fusion method, evaluation metric, number of sampled users, and the frequency of repeated recommendation for the baseline.


\subsection{Single Machine}
\label{subsec:eval-single-machine}

\begin{figure}[h!]
	\centering
	\input{plots/recommendation/single-partition}
	\caption{Evaluation of the MAP value for increasing K for 500 users}
	\label{plot:single-partition}
\end{figure}


\begin{figure}[h!]
	\centering
	\input{plots/recommendation/single-partition-boxplot}
	\caption{Different MAP values over multiple interations}
	\label{plot:single-partition-boxplot}
\end{figure}

\subsection{Multiple Machines}
\label{subsec:eval-multiple-machines}

\subsubsection{Hyperparameter evaluation results}
\label{subsubsec:eval-hyperparameter}
As explained in section \ref{subsec:hyperparameter-tuning}, this thesis investigates how different trained StarSpace models affect the partitioning of the data and, finally, the recommendation quality. Four main parameters were tuned during the training. These parameters can be found in the table \ref{tab:hyperparameter}. In the following table \ref{tab:models} you will find 16 models and the parameters with which they got trained.


The hyperparameter tuning benchmark was done on a two-partition setting to keep the evaluation simple. The hyperparameter tuning experiments were structured in the following steps:

\begin{enumerate}
    \item Train the StarSpace model with the training dataset and the parameters listed in each row of the table \ref{tab:models}.
    \item Calculate the projection matrix of the model (see section \ref{subsec:projection-matrix-calculation}).
    \item Load the model in memory and partition the test dataset on two partitions.
    \item Load sampled users (500, 2000) and generated the recommendations for each of them.
    \item Use the \emph{Most Interactions} data fusion approach to generate a single ranked list of recommendations for each user.
    \item Assess the observed ranking list with the baseline list and calculate the MAP@10.
\end{enumerate}


\begin{table}[!ht]
	\centering
	\caption{Trained model specifications}
	\label{tab:models}
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Model Nr.} & \textbf{Learning rate} & \textbf{Dimensions} & \textbf{Dropout probability} & \textbf{Text normalization} \\
        \hline
		\csvreader[
		late after line=\\\hline
		]{tables/hyperparameter-models-parameters.csv}
		{
            lr=\lr, 
            dim=\dim, 
            dropoutRHS=\dropoutRHS, 
            normalizeText=\normalizeText
		}
		{
		    \thecsvrow & \lr & \dropoutRHS & \dim & \normalizeText
		}%
	\end{tabular}
\end{table}


The experiments use the \emph{Most Interactions} data fusion approach. The reason is that this method was resulting in a better recommendation quality compared to the other two approaches when partitioning the data with the StarSpace model. The experiments were also done once with 500 and 2000 sampled users, respectively, to investigate the impact of the number of sampled users on the overall MAP@10 value.

Before diving into the recommendation quality benchmark of each model lets see how the different trained models impact the data distribution. In the diagram \ref{plot:edge-distribution} you can see how each model distributes the edges over two partitions. Compared to the data distribution of murmur2 in the diagram \ref{} the partitions are unbalanced here. 


\begin{figure}[h!]
	\centering
	\input{plots/recommendation/hyperparameter-data-distribution}
	\caption{Edge distribution of test dataset; sharded with different trained models over two partitions}
	\label{plot:edge-distribution}
\end{figure}


\subsubsection{Horizontall scalling}
\label{subsubsec:eval-horizontall-scalling}

