\section{Evaluation Pipeline to Assess Recommendations Quality}
\label{sec:evaluation-pipeline}
This work presents an evaluation pipeline to measure the impact of partitioning with different approaches on the recommendation quality. To build the pipeline each component in the section \ref{sec:eval-suite-architecture-components} is used. 

Figure \ref{fig:flowchart-evaluation-pipeline} indicates the flow of the pipeline from the start till the end. In the beginning, the users are sampled randomly. Next, the pipeline generates the baseline (also called Objective Ranking or Gold Standard). The step in the middle of the flowchart describes querying recommendations from the single partition instance to check the results' reproducibility; this evaluation is discussed in \ref{subsec:eval-single-machine}. 


The right side of the flowchart denotes the step where the recommendations of the multi-partition instances are queried. After that, an arbitrary data fusion approach reduces the result of the partitions to one list. In the last step, the pipeline starts comparing the results by calculating the evaluation metric configured. 

The main goal of the evaluation is to measure how far the observed ranking deviates from the objective ranking and keep the difference as small as possible. Moreover, this pipeline assesses how good or bad a trained StarSpace model performs during hyperparameter tuning (see section \ref{subsec:hyperparameter-tuning}).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{images/evaluation-flow-chart}
    \caption{Flowchart of the evaluation pipeline. First, an arbitrary number of users are randomly sampled. The left side of the flow chart highlights the baseline generation. The recommendations for the sampled users on the single partition instance are fetched \emph{N} times (for these experiments ten times), and the most appeared ones are stored. Then for observed rankings, the system requests the single and multiple partitions. Finally, the results are assessed against the baseline, and the MAP@K is calculated.}
    \label{fig:flowchart-evaluation-pipeline}
\end{figure}

The following sections will describe the evaluation metrics used to compute the recommendation quality and then go through the strategy to create a baseline and the observed ranking lists.

\subsection{Evaluation Metrics}
\label{subsec:evaluation-metrics-for-recommendation-quality}
The two main metrics used to evaluate the recommendations quality after partitioning the data are the \emph{Mean Average Precision at K (MAP@K)} (see section \ref{subsubsec:ap@k-map@k})  and \emph{Ranked-Biased Overlap (RBO)} (see section \ref{subsubsec:rbo}). 


With the mean average precision, it is possible to compare the objective ranking to the observed ranking without penalizing the ranking or the order of the results, making this metric ideal to see the impact of the partitioning on the recommendations.


In the ranked-biased overlap, the results at the top of the list have a higher value than the one at the bottom, so the comparison is much more strict, and differences in the top ranks are more penalized than differences at the bottom. RBO is an ideal metric to investigate the order of the returned results in different scenarios.


\subsection{Baseline Generation Strategy}
\label{subsec:baseline-generation}
The baseline serves as a crucial reference point for analyzing the changes and effects. It offers a foundation for comparing the situation before and after changes are applied to as a system. This thesis employs a partitioner to distribute a single instance system into multiple more minor instances. The partitioning should be done in a way that the different instance results are similar to the single instance. Therefore, the single instance's results are considered the baseline for assessing it against the merged results of the multiple-instance. But, there are two main issues in creating a baseline:

\begin{enumerate}
    \item The user feedback on their recommendations is missing from the crawled dataset (see section \ref{sec:data-crawling-strategy}). The lack of feedback on the data makes it difficult to create a baseline that includes both relevant and non-relevant data.
    
    \item The other issue is the non-deterministic behavior of random-walk-based recommender engines. Results can generally differ between random walks, meaning two engines operating on the same graph can return different recommendations, and even the same engine can yield different results for two consecutive requests for the same user.
\end{enumerate}

Assumptions are made to help overcome these problems. For the first issue, this work assumes that all the returned values from the single partition are relevant, and all other items in the corpus will be irrelevant. In practice, however, the recommendation engine produces relevant and non-relevant results. This assumption comes with its price, making the evaluation strict, especially when the algorithm is non-deterministic. Through the partitioning and classification of the data, new recommendations might appear for a given user. But these results will be assumed irrelevant, and evaluation matrics will yield low results, although the new recommendations could be of interest to the user.


Another solution to solve the first problem is to use \emph{Weak Supervision} approaches to employ labels on the dataset. Generate a labeling function using frameworks like \emph{Snorkel}\footnote{\url{https://snorkel.org/}} to classify the data as relevant or not relevant for a user. Thus, labeling the dataset is beyond the scope of this thesis.


For the second issue, this research proposes a novel idea: run the random walk for each user \emph{N} times and then take the most appeared recommendations. The intuition here is that gathering the most frequently suggested recommendation will filter the items picked few times by the random walk. Moreover, with this technique, we collect recommendations that appear even \emph{N} times, ensuring that these recommendations are important for a user. This approach will help to overcome the non-deterministic nature of the random-walk algorithm and crate a baseline top \emph{N} frequent results.


\subsection{Observed Rankings Generation Strategy}
\label{subsec:comparing-recommendtions}
The observed ranking is a list of recommendations that we compare against the baseline to evaluate the recommendation quality. The observed ranking list can be categorized into two groups:

\begin{enumerate}
    \item The observed ranking list of the single machine. This list helps to evaluate the reproducibility of the results. Besides, comparing the single partition results with the baseline determines how different the results for the same users are. 
    
    \item The observed ranking list produced by the multi-partition machine. Each partition produces a list of recommendations for a given user. The data fusion approach merges the results of the partitions into a single ranked list. 
\end{enumerate}
