\section{Evaluation Suite}
\label{sec:evaluation-suite}
The \emph{Evaluation Suite} evaluates the implementation and the different approaches proposed in this work \footnote{\url{https://github.com/ramin-master-thesis/Evaluation-Suite}}. The main purpose of the evaluation suite is to measure the recommendation quality of the distributed system. The evaluation suite compares the golden standard with the recommendations generated by the different data fusion approaches (explained in section \ref{sec:data-fusion-approaches}) based on the partitioning method used (see section \ref{sec:partitioning}). Moreover, the evaluation suite can measure the latency of the different system components. The evaluation suite is configurable and automates the evaluation process for different test scenarios. The following section will go through the overall architecture and explain each building block of the evaluation suite.


\subsection{Architecture and components}
\label{sec:eval-suite-architecture-components}
Figure \ref{fig:evaluation-suite-architecture} indicates an abstract architecture of the evaluation suite and its components. The system takes a configuration file and the list of users as its input. The configuration file provides different parameters to the system to create different evaluation scenarios. 


Initially, the evaluation suite randomly samples users. Then the evaluation suite generates a baseline (golden standard list), which is a list of recommendations for each sampled users. Next, the evaluation suite fetches the recommendations from the partitions, which are categorized as the observed rankings. Afterwards, the both ranked lists (objective and observed rankings) are compared against each other and the recommendation quality is measured. The suite also gathers meta-information (e.g., number of left index vertices, number of edges) from each partition. In the end, the system outputs the benchmark results. In the following, each component in figure \ref{fig:evaluation-suite-architecture} is described in detail.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.85\textwidth]{images/evaluation-suite-architecture}
    \caption{High level evaluation suite architecture and its components. The baseline generator and the recommendation fetcher communicate with the workers to retrieve the recommendations.}
    \label{fig:evaluation-suite-architecture}
\end{figure}


\paragraph{Sampler}
takes the number of samples as an input from the configuration file, samples the elements of the left index side (i.e., users), and then saves them in a file.

\paragraph{Baseline generator}
is responsible for generating the golden standard (i.e. objective rankings) as the baseline of the evaluation. This component only communicates with the single partition instance and creates the recommendations for the sampled users.

\paragraph{Recommendation fetcher}
communicates with each worker (i.e., partition) and sends the recommendation request of each randomly sampled entities to each worker. It then saves the recommendations in a file.

\paragraph{Recommendation quality calculator}
component implements the evaluation metrics \emph{MAP@K} and \emph{RBO} (see section \ref{subsec:evaluation-metrics}). These metrics are responsible for evaluating the recommendation quality.

Moreover, this component implements the data fusion approaches described in \ref{sec:data-fusion-approaches} and generates the single ranked list. This module compares the single ranked list with the baseline and measures the recommendation quality.


\paragraph{Partition status collector}
collects information about each partition. This information includes the partitioning method, partition-ID, and the number of edges, left and right vertices of the bipartite graph.

