\section{Data Crawling Strategy}
\label{sec:data-crawling-strategy}

GraphJet maintains a user-tweet bipartite graph. Creating this graph requires a real-world data set. In works like \cite{kwakWhatTwitterSocial2010} the authors study a massive dataset of tweets that they crawled. Twitter provides several APIs to access a variety of resources like tweets, users, and timelines. However, the Twitter API comes with its limitations. Twitter allows only sending 900 requests over any 15-minute interval\footnote{\url{https://developer.twitter.com/en/docs/twitter-api/rate-limits}}. 


This work uses an open-source Twitter crawler\footnote{\url{https://github.com/philipphager/twitter-crawler}} to gather data. The crawler uses multiple crawl processes in parallel with different API keys to make requests. Starting multiple crawling processes in parallel overcomes the API limitation of Twitter. Ten crawling processes were used to speed up the crawling by a factor of 10. 


At first, the crawler could only crawl user-ID, tweet-ID, and user-tweet interaction types (tweet, like, and retweet). This work extended the Twitter crawler. The extension also gathers the content of each tweet to the dataset. 


Elon Musk's profile, which has over 58 million followers, is the initial seed to start the crawl. The crawler crawls up to 25 tweets from the user's timeline. The crawler then adds the tweet author and 500 retweeters to its crawling queue for each tweet. This process continues until the crawler is stopped. Whenever a crawl process reaches the Twitter API endpoint limit, its data persists in a TSV file.


The authors of \cite{kwakWhatTwitterSocial2010} use different methods to clean the crawled data. This work does not use any specific data cleansing process during and after the crawl. However, the text normalization module of StarSpace (explained in sec \ref{subsubsec:star-space-text-normalization}) is used to normalize the text during partitioning. The normalization process only happens if the StraSpace model is trained with a normalized text parameter. More details about the text normalization impact on the partitioning and how it affects the recommendation quality can be found in the evaluation section \ref{subsec:hyperparameter-tuning}.


The StarSpace model needs a training dataset to train the model and a test dataset to evaluate the system. For a better evaluation quality, these two datasets should be different from each other. Therefore the crawling process was started two times in different periods. The training dataset was crawled between March 1st to March 5st. For the test dataset, the crawling started from March 10th to March 15st. Details of each dataset are shown in the tables \ref{tab:dataset}. These interactions between the users and tweets create the edges when generating the bipartite graph.

\begin{table}[!ht]
	\caption{Dataset specification}
	\label{tab:dataset}
	\begin{subtable}{.5\linewidth}
		\caption{Training Dataset}
		\centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            9,360,562 & 2,200,377 & 3,151,992	\\
            \hline
        \end{tabular}
	\end{subtable}%
	\begin{subtable}{.5\linewidth}
		\caption{Test Dataset}
		\centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            11,884,898 & 3,545,044 & 3,439,934	\\
            \hline
        \end{tabular}
	\end{subtable}%
\end{table}