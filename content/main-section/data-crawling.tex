\section{Data Crawling Strategy}
\label{sec:data-crawling-strategy}

GraphJet is developed at Tweeter to create real-time recommendations for a given user. GraphJet maintains a (user-tweet) bipartite graph in memory and traverses this graph to produce the recommendations. This study reaches the partitioning of the bipartite graph on multiple instances. Therefore, the single instances graph needs to be so massive that it can be sharded into multiple subgraphs. An enormous dataset is required in order to create such a dense graph.


Twitter is a massive document data source where users write and post tweets. In works like \cite{kwakWhatTwitterSocial2010} the authors study a massive dataset of tweets that they crawled. Twitter offers an Application Programming Interface (API) that is easy to crawl and collect data. However, the Twitter API comes with its limitations. Twitter allows only sending 900 requests over any 15-minute interval\footnote{\url{https://developer.twitter.com/en/docs/twitter-api/rate-limits}}. 


This thesis uses an open-source Twitter crawler\footnote{\url{https://github.com/philipphager/twitter-crawler}} to gather data. The crawler uses multiple crawl processes in parallel with different API keys to make requests. Despite the API limitations, the overall crawling process will not stop and continuously gather data from each started process.


At first, the crawler could only crawl user-ID, tweet-ID, and user-tweet interaction types (tweet, like, and retweet). This work extended the Twitter crawler. The extension also gathers the content of each tweet to the dataset. 


Elon Musk's profile, which has over 58 million followers, is the initial seed to start the crawl. The crawler crawls up to 25 tweets from the user's timeline. The crawler then adds the tweet author and 500 retweeters to its crawling queue for each tweet. This process continues until the crawler is stopped. Whenever a crawl process reaches the Twitter API endpoint limit, its data persists in a TSV file.


The authors of \cite{kwakWhatTwitterSocial2010} use different methods to clean the crawled data. This research does not use any specific data cleansing process during and after the crawl. However, the text normalization module of StarSpace (explained in sec \ref{subsubsec:star-space-text-normalization}) is used to normalize the text during partitioning. The normalization process only happens if the StraSpace model is trained with a normalized text parameter. More details about the text normalization impact on the partitioning and how it affects the recommendation quality can be found in the evaluation section \ref{subsec:hyperparameter-tuning}.


The StarSpace model needs a training dataset to train the model and a test dataset to evaluate the system. For a better evaluation quality, these two datasets should be different from each other. Therefore the crawling process was started two times in different periods. The training dataset was crawled between March 1st to March 5st. For the test dataset, the crawling started from March 10th to March 15st. Details of each dataset are shown in the tables \ref{tab:dataset}. These interactions between the users and tweets create the edges when generating the bipartite graph.

\begin{table}[!htb]
    \caption{Dataset specification}
    \label{tab:dataset}
    \begin{subtable}{.5\linewidth}
        \caption{Training Dataset}
        \centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            9,360,562 & 2,200,377 & 3,151,992   \\
            \hline
        \end{tabular}
    \end{subtable}%
    \begin{subtable}{.5\linewidth}
        \caption{Test Dataset}
        \centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            11,884,898 & 3,545,044 & 3,439,934  \\
            \hline
        \end{tabular}
    \end{subtable}%
\end{table}