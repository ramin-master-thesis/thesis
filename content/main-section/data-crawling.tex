\section{Data crawling strategy}
\label{sec:data-crawling-strategy}

GraphJet maintains a user-tweet bipartite graph. Creating this graph requires a real-world data set. In works like \cite{kwakWhatTwitterSocial2010} the authors study a massive dataset of tweets that they crawled. Twitter provides different APIs to access a variety of various resources like tweets, users, and timelines. It is important to notice that the Twitter API comes with its limitations. Twitter allows only sending 900 requests over any 15-minute interval. 


This work uses an open-source Twitter crawler\footnote{\url{https://github.com/philipphager/twitter-crawler}} to gather data. The crawler uses multiple crawl processes in parallel with different API keys to make requests. Starting multiple crawling processes in parallel overcomes the API limitation of Twitter. I used ten crawling processes to speed up the crawling by a factor of 10. 


The crawler could only crawl user-ID, tweet-ID, and user-tweet interaction types (tweet, like, and retweet). This work extended the Twitter crawler. The extension also adds the content of each tweet to the final dataset. 


Elon Musk's profile, which has over 58 million followers, is the initial seed to start the crawl. The crawler crawls up to 25 tweets from the user's timeline. The crawler then adds the tweet author and up to 500 retweeters to its crawling queue for each tweet. This process continues until the crawler is stopped. Whenever a crawl process reaches the Twitter API endpoint limit, its data persists in a TSV file.


The authors of \cite{kwakWhatTwitterSocial2010} use different methods to clean the crawled data. This work does not use any specific data cleansing process during and after the crawl. I used the text normalization module of StarSpace (explained in sec \ref{subsubsec:star-space-text-normalization}) to normalize the text during partitioning. Regardless of the dataset and its content quality, this approach normalizes the content for the partitioner. The normalization process only happens if the StraSpace model is trained with a normalized text parameter. More details about the text normalization impact on the partitioning and how it affects the recommendation quality can be found in the evaluation section \ref{subsec:hyperparameter-tuning}.


The StarSpace model needs a training dataset to train the model and a test dataset to evaluate the system. For a better evaluation quality, these two datasets should be different from each other. Therefore the crawling process was started two times in different periods. The training dataset was crawled between March 1st to March 5st, and for the test, the dataset was from March 10th to March 15st. Details of each dataset are shown in the tables \ref{tab:dataset}. With the help of the crawled dataset, we can build the bipartite graph. These interactions between the users and tweets create the edges when generating the bipartite graph.


\begin{table}[!h]
	\caption{Dataset specification}
	\label{tab:dataset}
	\begin{subtable}{.5\linewidth}
		\caption{Training Dataset}
		\centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            9,360,562 & 2,200,377 & 3,151,992	\\
            \hline
        \end{tabular}
	\end{subtable}%
	\begin{subtable}{.5\linewidth}
		\caption{Test Dataset}
		\centering
        \begin{tabular}{|l|c|r|m{0.4\linewidth}|}
            \hline
            \textbf{Interactions} & \textbf{Users} & \textbf{Tweets} \\
            \hline
            11,884,898 & 3,545,044 & 3,439,934	\\
            \hline
        \end{tabular}
	\end{subtable}%
\end{table}