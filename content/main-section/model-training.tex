\section{Model Training}
\label{sec:model-training}
The StarSpace model is trained to be able to predict the embedding vectors of the new documents. To train the StarSpace model train data needed to be generated fromat that the model can consume. As explained in the section \ref{subsec:partitioning-star-space} to calculate the partition-ID, the StarSpace model needs the a projection matrix to reduce the dimensions of the predicted embedding vector. Figure \ref{fig:star-space-training} shows the flowchart of the training and projection matrix calculation process. At the beginign, related documents to each other are generated using the recommendation algorithm. The data is then used to train the model with the configured hyperparameters. After the training process is finished the projection matrix is calculated and saved for later use.

In the following sections, each step of the flowchart \ref{fig:star-space-training} is explained in detail.

\begin{figure}[!h]
	\centering
	\includegraphics[height=0.75\textwidth]{images/train-StarSpace-flow-chart}
	\caption{Flowchart of the training process}
	\label{fig:star-space-training}
\end{figure}

\subsection{Generating Training Data}
\label{subsec:generating-training-data}
The first step to train the StarSpace model is to generate train data. The train input file that StarSpace takes is a text file and has a special structure. Each line of the file is considered as a document. Each document is build with sentences and each sentence is sperated by a tab from each other. With this format StarSpace learns the mapping between the sentecnes and documents and is able to predict the related documents for a given sentece. Moreover, StarSpace would be able to calculate the embedding vecotor for a given sentece.


The train dataset used in this work to train StarSpace is crawled from Twitter directly (see section \ref{sec:data-crawling-approach}). This dataset needs to be formated to be digestable for StarSpace. StarSpace needs in each line multiple tweets related to each other that can be defined as a document. We can argue that each document in this file is build with sentences that are related to each other. This works proposes an approach in which the SALSA algorithm to find related senteces to each other.


As explained in the subsection \ref{subsub:storage-index-layer} this work extends the SALSA algorithm by starting the random walk from the right side index (tweet side) and return the vertix list of the right side nodes (tweets). The intution here is that SALSA produces related tweets to the tweet, which the walk started on. Therefore, the train data can be loaded into memory and by generating the bipartite graph and start to generate for each tweet, the related tweets. Then each tweet and its related tweets are writen into a text file with seperated tabs. This process continues for all the tweets.


The algorithm \ref{alg:star-space-train-data} explains how the data generation process works. At the beginign all the identifiers from the right side index (tweet-IDs) are loaded. Then for each id the \emph{getRelatedIDs} function runs the SALSA walk and calculates the related node-IDs (tweets) for the given identifier with a cut-off of ten. The input identifier is also added to the end result list. After that the content of each identifier is retrieved from the identifier-content index and with tab seperated joined together as a string. This will produce one document and this particular document will be saved in a text file. The algorithm continues until all the identifiers finish.


\begin{algorithm}[H]
    \caption{StarSpace train data generation algorithm}
    \label{alg:star-space-train-data}
    \SetKwData{allIDs}{allIDs}
    \SetKwData{id}{id}
    \SetKwData{listRecommendations}{listRecommendations}
    \SetKwData{listContent}{listContent}
    \SetKwData{document}{document}


    \SetKwFunction{getAllRightIndexKeys}{getAllRightIndexKeys}
    \SetKwFunction{getRelatedIDs}{getRelatedIDs}
    \SetKwFunction{getContentOf}{getContentOf}
    \SetKwFunction{joinWithTab}{joinWithTab}
    \SetKwFunction{appendToTextFile}{appendToTextFile}

    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}

    \Output{Train data in a text file }
    \SetAlgoLined

    \BlankLine\emph{//initialization}\BlankLine
    \allIDs $\leftarrow$ \getAllRightIndexKeys{}\;

    \BlankLine

    \ForEach{identifier $\id$ in $\allIDs$}
    {
		\emph{//Start the random walk from the right index side with node-ID id}\;
		\listRecommendations $\leftarrow$ \getRelatedIDs{\id}\;
		\BlankLine
		\emph{//Append the initial id to the results at the beginign of the resutls}\;
		\listRecommendations.insert(0, \id)\;
		\BlankLine
		\listContent $\leftarrow$ \getContentOf{\listRecommendations}\;
		\document $\leftarrow$ \joinWithTab{listContent}\;
		\appendToTextFile{document}\;
    }

    \BlankLine
\end{algorithm}


As an example on how good the SALSA algorithm generates related tweets, take a look at the table \ref{tab:example-train-data}. This table shows the recommendations generated from the SpaceX tweet with the following content: "Live views of Starship SN8’s flight test -> https://t.co/Hs5C53qBxb https://t.co/OvAloTO6UQ"

\begin{table}[!h]
	\centering
	\caption{Example output of similar tweets for train data generation}
	\label{tab:example-train-data}
	\begin{tabular}{|m{0.95\textwidth}|}
		\hline
		\textbf{Recommendations} \\
		\hline
		Welcome to Mars, @NASAPersevere! Congratulations @NASA and @NASAJPL – another successful mission to the Red Planet https://t.co/EtVb2f2C5I \\
		\hline
		Tesla’s bold "yoke" steering system is legal on UK roads, confirms regulators \\
		\hline
		@Erdayastronaut @ErcXspace Short-term, CH4 delivered \& O2 produced. Propellant is ~78\% O2. Long-term, Sabatier reaction to convert CO2 + H2O -> CH4 + O2 using wind \& solar power. \\
		\hline
		@elonmusk Well done @elonmusk and the team, keep pushing those boundaries and ignore the competition,  you have enough vision to see the finish line whilst they are struggling to find the start! (Remember 2MWs ultracapacitor in the frunk option for 100\% regen in any temp and the win!) \\
		\hline
		@elonmusk @NASASpaceflight 250 tons would be nice and easy for even engine scaling , since you are using odd numbered the goal has to be 333 tons \\
		\hline
		@Erdayastronaut Great footage Tim and the team. The audio was great also,  the kids were covering their ears during launch!! \\
		\hline
		@elonmusk @Teslarati Totally agree, the new Lotus ev has additional active damping for longitudinal acceleration as well... \\
		\hline
		Feel your pain @SpaceX , it's only 5 year old homebrew but I shall name it SN1 https://t.co/1sf1MzXaH3 \\
		\hline
		\#LaunchThePengwing The naughty Elves are not clear of the pad! Thank you Felix and team. https://t.co/UF7FB8P4OL \\
		\hline
		@elonmusk Totally agree \\
		\hline
	\end{tabular}
\end{table}


\subsection{Projection Matrix Calculation}
\label{subsec:projection-matrix-calculation}
After the train data is generated the StarSpace model can be trained. The training gets initialized by the train parameters and the train file. When the training finishes and the model is saved, the calculation of the projection matrix starts. The algorithm \ref{alg:projection-matrix-calc} shows how the computation is done.

\begin{algorithm}[H]
    \caption{Projection matrix calculation}
    \label{alg:projection-matrix-calc}
    \SetKwData{model}{model}
    \SetKwData{allSentences}{allSentences}
    \SetKwData{sen}{sen}


    \SetKwFunction{loadModel}{loadModel}
    \SetKwFunction{getAllSentencesFromTrainDataset}{getAllSentencesFromTrainDataset}
    \SetKwFunction{computeEigenVector}{computeEigenVector}

    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{output}

    \Output{Projection matrix of model \emph{M}}
    \SetAlgoLined

    \BlankLine\emph{//initialization}\BlankLine
    \model $\leftarrow$ \loadModel{}\;
    \allSentences $\leftarrow$ \getAllSentencesFromTrainDataset{}\;

    \BlankLine

    \lForEach{sentence $\sen$ in $\allSentences$} { X $\leftarrow$ \model.getDocVector(\sen) }

	(eigenvalue, eigenvector) $\leftarrow$ \computeEigenVector{X}

    \BlankLine
\end{algorithm}

\todo{Do I really need to go through all the mathematical process of the projection matrix calculation?}

\subsection{Hayperparameter Tuning}
\label{subsec:hyperparameter-tuning}
The model of StarSpace has multiple parameters. The parameters tune the model for a better prediction and model depending on the train data set. So it is important to tune these hyperparameters for better model prediction. A complete documentation of parameters could be found in StarSpace repository on GitHub. \footnote{\url{https://github.com/facebookresearch/StarSpace\#full-documentation-of-parameters}}

\begin{table}[!h]
	\centering
	\caption{Variation of StarSpace Hyperparameter}
	\label{tab:hyperparameter-variations}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Parameter Name} & \textbf{Values} \\
		\hline
		lr & [0.01, 0.05] \\
		\hline
		dim & [100, 300] \\
		\hline
		dropoutRHS & [0.5, 0.8] \\
		\hline
		normalizeText & [True, False]\\
		\hline
	\end{tabular}
\end{table}

Table \ref{tab:hyperparameter-variations} shows different hyperparameters values used to train the StarSpace model. The train criteria is based on the MAP@10 value of the union result, meaning if a model has a higher MAP@10 value comparing to the other models it is the better model.


Figure \todo{here comes the MAP@10 figure of the different models} shows the different MAP@10 values of different models.

