\section{Model Training}
\label{sec:model-training}
To train the StarSpace model train data needed to be generated fromat, the model can be trained. Figure \ref{fig:star-space-training} shows the flowchart of the training process. First, in this section, I describe how the training data is generated, and in the next part, the hyperparameter tuning of the model is described.

\begin{figure}[!h]
	\centering
	\includegraphics[height=0.50\textwidth]{images/train-StarSpace-flow-chart}
	\caption{Flowchart of the training process}
	\label{fig:star-space-training}
\end{figure}

\subsection{Generating Training Data}
\label{subsec:generating-training-data}
I used the SALSA algorithm to generate the training data. Table \ref{tab:train-data} shows how big the training dataset is. The dataset is indexed as a bipartite graph and then for each tweet the recommendations are calculated. In other words, instead of quering recommendations for a user, the random walk starts on the right-hand side of the bipartite graph. The output is a ranked list of recommendations for a specific tweet. Then the queried tweet among with the other output are written in a text file. The format of the file is explained in section \ref{StarSpace}. This process continues for each node on the right-hand side.



As an example take a look at the table \ref{tab:example-train-data}. This table shows the recommendations generated from the SpaceX tweet with the following content: "Live views of Starship SN8’s flight test -> https://t.co/Hs5C53qBxb https://t.co/OvAloTO6UQ"

\begin{table}[!h]
	\centering
	\caption{Example output of similar tweets for train data generation}
	\label{tab:example-train-data}
	\begin{tabular}{|m{0.95\textwidth}|}
		\hline
		\textbf{Recommendations} \\
		\hline
		Welcome to Mars, @NASAPersevere! Congratulations @NASA and @NASAJPL – another successful mission to the Red Planet https://t.co/EtVb2f2C5I \\
		\hline
		Tesla’s bold "yoke" steering system is legal on UK roads, confirms regulators \\
		\hline
		@Erdayastronaut @ErcXspace Short-term, CH4 delivered \& O2 produced. Propellant is ~78\% O2. Long-term, Sabatier reaction to convert CO2 + H2O -> CH4 + O2 using wind \& solar power. \\
		\hline
		@elonmusk Well done @elonmusk and the team, keep pushing those boundaries and ignore the competition,  you have enough vision to see the finish line whilst they are struggling to find the start! (Remember 2MWs ultracapacitor in the frunk option for 100\% regen in any temp and the win!) \\
		\hline
		@elonmusk @NASASpaceflight 250 tons would be nice and easy for even engine scaling , since you are using odd numbered the goal has to be 333 tons \\
		\hline
		@Erdayastronaut Great footage Tim and the team. The audio was great also,  the kids were covering their ears during launch!! \\
		\hline
		@elonmusk @Teslarati Totally agree, the new Lotus ev has additional active damping for longitudinal acceleration as well... \\
		\hline
		Feel your pain @SpaceX , it's only 5 year old homebrew but I shall name it SN1 https://t.co/1sf1MzXaH3 \\
		\hline
		\#LaunchThePengwing The naughty Elves are not clear of the pad! Thank you Felix and team. https://t.co/UF7FB8P4OL \\
		\hline
		@elonmusk Totally agree \\
		\hline
	\end{tabular}
\end{table}


\subsection{Projection Matrix Calculation}
\label{subsec:projection-matrix-calculation}


\subsection{Hayperparameter Tuning}
\label{subsec:hyperparameter-tuning}
The model of StarSpace has multiple parameters. The parameters tune the model for a better prediction and model depending on the train data set. So it is important to tune these hyperparameters for better model prediction. A complete documentation of parameters could be found in StarSpace repository on GitHub. \footnote{\url{https://github.com/facebookresearch/StarSpace\#full-documentation-of-parameters}}

\begin{table}[!h]
	\centering
	\caption{Variation of StarSpace Hyperparameter}
	\label{tab:hyperparameter-variations}
	\begin{tabular}{|l|c|}
		\hline
		\textbf{Parameter Name} & \textbf{Values} \\
		\hline
		lr & [0.01, 0.05] \\
		\hline
		dim & [100, 300] \\
		\hline
		dropoutRHS & [0.5, 0.8] \\
		\hline
		normalizeText & [True, False]\\
		\hline
	\end{tabular}
\end{table}

Table \ref{tab:hyperparameter-variations} shows different hyperparameters values used to train the StarSpace model. The train criteria is based on the MAP@10 value of the union result, meaning if a model has a higher MAP@10 value comparing to the other models it is the better model.


Figure \todo{here comes the MAP@10 figure of the different models} shows the different MAP@10 values of different models.

