\chapter{Introduction}

In big data systems, the growth of data over time is inevitable. While the growth of data drives the urge to scale the system vertically, this scaling method has the inherent issue of making deployment and maintenance complicated and expensive. Limited scaling, more extended downtime, and greater risk of an outage are drawbacks of vertical scaling. On the other hand, horizontal scaling is more cost-efficient and can be achieved through parallelism. Parallel processing is the technique of dividing tasks among a group of resources. This division of work enables horizontal scaling of the system.


Parallelism provides advantages like higher throughput, lower latency by employing further computational resources, and additionally, resource waste is minimized by scaling up and down workers based on the actual workload. But parallelism also introduces a new set of complicated challenges to address. The majority of these issues are software-related. Many distributed parallel programming middlewares such as OpenMP aim to ease creating multi-threaded programs on each node. Moreover, other programming models like massage parsing toolkits (e.g., OpenMPI and AKKA) distribute the algorithm across the nodes. Computer scientists must adopt the existing procedures and algorithms to the parallel execution model. However, this adaption is often inefficient, especially when the algorithm demands frequent expensive network calls between the nodes \cite{sharmaGraphJetRealtimeContent2016, eksombatchaiPixieSystemRecommending2018}.


Another approach to obtain parallelism is through data distribution. By partitioning the data and sending them on multiple nodes, we apply a shared-nothing architecture, where each node processes the data in complete isolation. Thus, in document processing systems, i.e., recommendation systems, partitioning the data in a clean way is challenging due to the high dependency between the documents in the corpus dataset. Therefore, these systems rely on a single instance deployment and avoid any distribution \cite{guptaWTFWhoFollow2013,sharmaGraphJetRealtimeContent2016, eksombatchaiPixieSystemRecommending2018}. It is evident that the document corpus grows over time, which overloads the single instance and leads to a potential single point of failure. Therefore, these systems replicate the workers and balance the request load on the replicated instances to guarantee availability.



The aim is to develop a more sophisticated method for distributing the data only by observing the input and the output and considering the system as a black box. Furthermore, this method provides an adaptive partition middleware that allows horizontal scaling without parallelizing the system's algorithm and still delivers "good enough" results. 


With this aim in mind, this thesis presents a new method for partitioning a stream of documents based on a trained similarity measure. An embedding model called StarSpace \cite{wuStarSpaceEmbedAll2017} is at the heart of the proposed technique, which reads the content of the documents and computes embedding vectors. Used as a representation of the documents, these embedding vectors are being used to denote relevant documents to land "near" each other in a so-called embedding space. Combining this method with dimension-reduction methods such as \emph{Principal Component Analysis (PCA)} \cite{woldPrincipalComponentAnalysis1987} enables calculating a partition-ID for each upcoming document. Whenever a request is sent, and each worker yields its results, these results are gathered together using \emph{Data Fusion} methods. Various information retrieval metrics are studied and discussed to assess the output result of the multi-instance environment against the single-instance environment.


GraphJet \cite{sharmaGraphJetRealtimeContent2016}, a real-time recommendation engine developed by Twitter, has been chosen to test the researched technique. This recommender system uses the user-tweet interactions to create a bipartite graph in memory. The recommendation engine runs a random-walk-algorithm called \emph{Stochastic Approach for Link-Structure Analysis (SALSA)} \cite{lempelSALSAStochasticApproach2001} to generate recommendations for a given user. 


While GraphJet hypothetically can be distributed, the authors decided against this due to frequent expensive network calls between the machines. Thus, this thesis has been developed based on the assumption that each worker (machine or partition) runs the random walk algorithm in complete isolation and yields its results based on the data shard it maintains. To design an appropriate methodology, this research is focused on a real-world user-tweet dataset that has been gathered using a crawler. 


This thesis implements a prototype of GraphJet simulating the simplified indexing system and recommendation engine. This prototype loads the dataset into memory and maintains a bipartite graph. For the multi-instance environment, this prototype loads a fraction of the data it receives from the partitioning module on each instance. The partitioning module implements different partitioning methods: a random partitioning method (Murmur2) for comparison purposes and the primary content-based partitioning approach.


The SALSA algorithm is used to find the relevant documents for training the StarSpace model. Later in the process, a projection matrix is calculated for the trained model to reduce the dimensionality of the predicted embedding vectors. The train data generation and the projection matrix calculation are the building blocks of a hyperparameter tuning pipeline to train future StarSpace models.


In a multi-partition environment, each worker produces a list of recommendations for an incoming query. These results need to get merged into a single ranked list of recommendations to get benchmarked against the results of the single-instance output. Therefore, three novel \emph{Data Fusion} strategies are presented to combine or choose between the generated recommendation lists.


Finally, the evaluation suite implemented in this research aims to measure how far the distributed system results (i.e., recommendations) deviate from the single partition results. This difference is computed by metrics used to benchmark recommendations systems. In the evaluation stage, various test scenarios are investigated based on the choice of the data fusion strategy, partitioning method, and the number of partitions. An evaluation pipeline is introduced to simulate all the possible scenarios, which can then be assessed to one another to find the superior choice of method.


This thesis is organized as follows. Chapter \ref{chap:fundamentals} aims to look into and classify the relevant technologies, introduce essential concepts, and define terminology, setting the scene for the following sections. In chapter \ref{chap:concept} the research idea is formally structured by an in-depth exploration of the research problem. In chapter \ref{chap:evaluation} the evaluation suite, which is a crucial part of evaluating the system's performance, is closely studied. Finally, in chapter \ref{chap:conclusion} the paper is concluded by mentioning the observations made from analyses of results, plus the future outlook of the research directions and critical developments are discussed.
