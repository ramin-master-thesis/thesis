\chapter{Introduction}

In big data systems, the growth of data over time is inevitable. While the growth of data drives the urge to scale the system vertically, this scaling method has the inherent issue of making deployment and maintenance complicated and expensive. Limited scaling, more extended downtime, and greater risk of an outage are drawbacks of vertical scaling. On the other hand, horizontal scaling is more cost-efficient and can be achieved through parallelism.


Parallel processing is the technique of dividing tasks among a group of resources. Parallelism provides advantages like higher throughput, lower latency by employing further computational resources, and additionally, resource waste gets minimized by scaling up and down workers based on the actual workload. But parallelism also introduces a new set of complicated challenges to address. The majority of these issues are software-related. Many distributed parallel programming middlewares such as OpenMP aim to ease creating multi-threaded programs on each node. Moreover, other programming models like massage parsing toolkits (e.g., OpenMPI and AKKA) distribute the algorithm across the nodes. Computer scientists must adopt the existing procedures and algorithms to the parallel execution model. However, this adaption is often inefficient, especially when the algorithm demands frequent expensive network calls between the nodes \cite{eksombatchaiPixieSystemRecommending2018, sharmaGraphJetRealtimeContent2016}.


Another approach to obtain parallelism is through data distribution. By partitioning the data and sending them on multiple nodes, we apply a shared-nothing architecture, where each node processes the data in complete isolation. Thus, in document processing systems, i.e., recommendation systems, partitioning the data in a clean way is challenging due to the high dependency between the documents in the corpus. Therefore, these systems rely on a single instance deployment and avoid any distribution \cite{ eksombatchaiPixieSystemRecommending2018, guptaWTFWhoFollow2013,sharmaGraphJetRealtimeContent2016}. It is evident that the document corpus grows over time, which overloads the single instance and leads to a potential single point of failure. Therefore, these systems replicate the workers and balance the request load on the replicated instances to guarantee availability.



The aim is to develop a more sophisticated method for distributing the data only by observing the input and the output and considering the system as a "black box." Furthermore, this method provides an adaptive partition middleware that allows horizontal scaling without parallelizing the system's algorithm and still delivers "good enough" results. 


With this aim in mind, this thesis presents a new method for partitioning a stream of documents based on a trained similarity measure. An embedding model called StarSpace \cite{wuStarSpaceEmbedAll2017} is at the heart of the proposed technique, which reads the content of the documents and computes embedding vectors. Used as a representation of the documents, these embedding vectors are being used to denote relevant documents to land "near" each other in a so-called embedding space. 


Whenever the system receives a write request, the pre-trained StarSpace model in the partitioning module calculates the embedding vector of the given document, which can result in a massive dimension. Thus, with the aid of dimension-reduction methods such as \emph{Principal Component Analysis (PCA)} \cite{woldPrincipalComponentAnalysis1987}, this thesis introduces a method to calculate a partition-ID for the arriving documents. 


During read time, the query is forwarded to all the workers. After the workers process the query, each of them yields their results, which are gathered together using \emph{Data Fusion} methods. Three novel data fusion strategies are presented to combine or choose between the generated results. Furthermore, various information retrieval metrics are studied and discussed to assess the output result of the multi-instance environment against the single-instance environment.


GraphJet \cite{sharmaGraphJetRealtimeContent2016}, a real-time recommendation engine developed by Twitter, has been chosen to test the researched technique. This recommender system uses the user-tweet interactions to create a bipartite graph in memory. The recommendation engine runs a random-walk-algorithm called \emph{Stochastic Approach for Link-Structure Analysis (SALSA)} \cite{lempelSALSAStochasticApproach2001} to generate recommendations for a given user. 


While GraphJet hypothetically can be distributed, the authors decided against this due to frequent expensive network calls between the machines. Thus, this thesis has been developed based on the assumption that each worker (machine or partition) runs the random walk algorithm in complete isolation and yields its results based on the data shard it maintains. To design an appropriate methodology, this research is focused on a real-world user-tweet dataset that has been gathered using a crawler. 


This thesis implements a prototype of GraphJet that simulates the simplified indexing system and recommendation engine. This prototype loads the dataset into memory and maintains a bipartite graph. For the multi-instance environment, this prototype loads a fraction of the data it receives from the partitioning module on each instance. The partitioning module implements different partitioning methods: First, a random partitioning method (Murmur2) for comparison purposes, and second, the content-based partitioning approach introduced in this thesis.


The SALSA algorithm is used to find the relevant documents for training the StarSpace model. Later in the process, a projection matrix is calculated for the trained model to reduce the dimensionality of the predicted embedding vectors. The training data generation and the projection matrix calculation are the building blocks of a hyperparameter tuning pipeline to train future StarSpace models.


Figure \ref{fig:introduction} demonstrates the research idea for a recommendation system. The user interacts with the system, for example, by writing or liking documents (i.e., tweets), which tunes the recommendation engine. The purpose of a recommendation engine is to find relevant items for a given user. The recommendation algorithm finds the relevancy between the items, which can be applied to generated training data, and digests them to the partitioner model. The model learns the dependency between the suggested data and can partition upcoming documents based on semantic similarity.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.50\textwidth]{images/introduction}
    \caption{A layered model describing the overall idea of the proposed approach in this thesis. The user interacts with the system, which tunes the recommendation engine. The partitioner uses the output of the recommendation engine to learn the similarity between the data.}
    \label{fig:introduction}
\end{figure}


Finally, the evaluation suite implemented in this research aims to measure how far the distributed system results (i.e., recommendations) deviate from the single partition results. This difference is computed by metrics used to benchmark recommendations systems. In the evaluation stage, various test scenarios are investigated based on the choice of the data fusion strategy, partitioning method, and the number of partitions. An evaluation pipeline is introduced to simulate all the possible scenarios, which can then be assessed to one another to find the superior choice of method.


This thesis is organized as follows. Chapter \ref{chap:fundamentals} aims to look into and classify the relevant technologies, introduce essential concepts, and define terminology, setting the scene for the following sections. In chapter \ref{chap:concept} the research idea is formally structured by an in-depth exploration of the research problem. In chapter \ref{chap:evaluation} the evaluation suite, which is a crucial part of evaluating the system's performance, is closely studied. Finally, in chapter \ref{chap:conclusion} the paper is concluded by mentioning the observations made from analyses of results, plus the future outlook of the research directions and critical developments are discussed.
