\chapter{Introduction}

With the recent developments in computer science, horizontal scaling has become a factor of increasing importance in distributed systems design. Many systems, such as document processing systems, rely on a single instance deployment. These systems take a corpus of documents as input and send them one by one to a worker. Then the worker processes the incoming documents. Over time the document corpus grows, which leads to an overload of the worker and makes it a potential single point of failure.


These systems rely on replicating the workers and distributing the request load on the replicated instances to guarantee availability. With the growth of data, the urge to scale the system vertically arises. Vertical scaling has the inherent issue of making deployment and maintenance complicated and expensive. On the contrary, horizontal scaling will distribute a fraction of data on different workers, which is more cost-efficient than vertical scaling. 


With the aid of this approach, the data is spread across multiple workers. Each worker will hold and maintain a fraction of the document corpus. A new worker can be added to the cluster whenever the data grows, taking off the load. This process of the partitioning of data makes it possible to scale the system horizontally. The data reduction on each worker also provides other advantages like less memory usage and faster processing of the documents.


This thesis proposes a partitioning technique based on the topic of the incoming text document to distribute text document data on multiple workers. 
At the heart of the proposed \emph{content-based} partitioning technique is an embedding model called \emph{StarSpace}, which reads the content of the documents and computes embedding vectors. These embedding vectors denote that documents similar to each other will land "near" each other in a so-called \emph{embedding space}. The embedding vector is used as a representation of the documents. Using dimension-reduction methods such as PCA allows us to calculate a partition-ID for each upcoming document. In other words, documents with a similar topic get classified together and land on the same partition.



This work builds upon the work of GraphJet, a real-time recommendation engine developed by Twitter to study the proposed approach. This recommender system uses the user-tweet interactions to create a bipartite graph in memory. The recommendation engine runs a random-walk-algorithm called \emph{Stochastic Approach for Link-Structure Analysis (SALSA)} to generate recommendations for a given user. GraphJet can hypothetically be distributed, but the authors decided against this due to expensive network calls between the machines. Therefore this study assumes that each worker (aka machine or partition) runs the random walk algorithm in complete isolation and yields its own results based on the data shard it maintains. 


This thesis uses a real-world user-tweet dataset. The dataset has been gathered using a crawler. Initially, the prototype implemented for this research simulates the indexing system of GraphJet on a single instance. Afterward, the thesis introduces a novel partitioning module. A state-of-the-art partitioning method (Murmur2) is used for comparison purposes against the primary proposed partitioning approach.


This thesis leverages the SALSA algorithm to generate the StarSpace model's training data and then calculates the projection matrix for the trained model. Furthermore, this pipeline is used as a building block for hyperparameter tuning of future models. 


In a multi-partition environment, each worker produces a list of recommendations for an incoming query. These results need to get merged into a single ranked list of recommendations to get benchmarked against the results of the single-instance output. Therefore, this thesis presents three novel \emph{data fusion} strategies to combine or choose between the generated recommendation lists.


The evaluation suite, implemented in this research, aims to measure how far the distributed system results (i.e., recommendations) deviate from the single partition results. This difference is computed by metrics used to benchmark recommendations systems. In the evaluation stage, the author looks at various possibilities and different test scenarios based on the choice of the data fusion strategy, partitioning method, and the number of partitions. An evaluation pipeline is introduced to simulate all the possible scenarios, which can then be assessed to one another to find the superior choice of method.


% The evaluation at the end shows that the "content-based" partitioning approach delivers a better result compared to the random partitioning method. Moreover, the suite assesses the latency of each worker. The results of the experiments show improvement in recommendation calculation time. Besides, by increasing the number of workers, the latency improves exponentially.
 