\chapter{Introduction}

Parallel processing is a rapidly evolving technology with applications in various computer science and engineering fields. It is only natural that the concurrencies in many application domains start to reflect in computer systems. We live in a complex and concurrent environment, and when we get a greater understanding of computer technology, we tend to bring that concurrency to our computers. With the advent of computer science, supercomputers and smaller computers adopt parallel processing methods more and more. Parallel programming is the technique of division tasks among a group of resources. This division of taks enables horizontal scaling of the system.


Parallelism provides unrivaled speed and cost-effectiveness, but it also introduces a new set of complicated challenges to address. The majority of these issues are software-related. Many distributed parallel programming middlewares such as OpenMP aim to ease creating multi-threaded programs on each node. Moreover, other programming models like massage parsing toolkits (e.g., OpenMPI and AKKA) tend to distribute the algorithm across the nodes. Computer scientists must adopt the existing procedures and algorithms to the parallel execution model. However, this adaption is often inefficient, especially when the algorithm demands frequent expensive network calls between the nodes.


Another approach to creating parallelism is through data distribution parallelization. By partitioning the data and sending them on an instance, we established an isolated environment for each instance to process the data. Thus, in document processing systems, i.e., recommendation systems, partitioning the data in a clean way is challenging due to the high dependency between the documents in the corpus dataset. Therefore, these systems rely on a single instance deployment and avoid any distribution. It is evident that the document corpus grows over time, which overloads the single instance and leads to a potential single point of failure. These systems rely on replicating the workers and distributing the request load on the replicated instances to guarantee availability to overcome such failure.


While the growth of data drives the urge to scale the system vertically, this scaling method has the inherent issue of making deployment and maintenance complicated and expensive: Limited scaling, more extended downtime, and greater risk of an outage. On the other hand, horizontal scaling is more cost-efficient by distributing a fraction of the document corpus across multiple workers.


With respect to the relationship the documents have with each other, this thesis proposes a partitioning technique to distribute text document data on multiple workers by their \emph{similarity} to each other. This approach will keep the documents that are related to each other close. An embedding model called StarSpace is at the heart of the proposed technique; Which reads the content of the documents and computes embedding vectors. Used as a representation of the documents, these embedding vectors are being used to denote similar documents to land "near" each other in a so-called embedding space. Also, using dimension-reduction methods such as \emph{Principal Component Analysis (PCA)} enables calculating a partition-ID for each upcoming document. After each partition yields its results, these results are gathered together using \emph{Data Fusion} methods. 


This approach observes the input and the output and considers the system as a black box. Furthermore, this method provides an adaptive partition middleware that allows for horizontal scaling without parallelizing the system's algorithm and still delivers "good enough" results.


GraphJet, a real-time recommendation engine developed by Twitter, has been chosen to test the researched technique. This recommender system uses the user-tweet interactions to create a bipartite graph in memory. The recommendation engine runs a random-walk-algorithm called \emph{Stochastic Approach for Link-Structure Analysis (SALSA)} to generate recommendations for a given user. 


While GraphJet hypothetically can be distributed, the authors decided against this due to expensive network calls between the machines. Thus, this study has been developed based on the assumption that each worker (machine or partition) runs the random walk algorithm in complete isolation and yields its results based on the data shard it maintains. To design an appropriate methodology, this research is focused on a real-world user-tweet dataset that has been gathered using a crawler. 


This study implements a prototype of GraphJet simulating the simplified indexing system and recommendation engine. The prototype loads the dataset in memory and maintains a bipartite graph. For the multi-instance environment, this prototype loads a fraction of the data it receives from the partitioning module on each instance. The partitioning module implements different partitioning methods. Namely, a baseline partitioning method (Murmur2) is used for comparison purposes against the primary (content-based partitioning) proposed approach. The SALSA algorithm has been used to generate the StarSpace model's training data. After the projection matrix for the trained model is calculated. This pipeline is used as a building block for hyperparameter tuning of future StarSpace models.


In a multi-partition environment, each worker produces a list of recommendations for an incoming query. These results need to get merged into a single ranked list of recommendations to get benchmarked against the results of the single-instance output. Therefore, three novel data fusion strategies have been presented to combine or choose between the generated recommendation lists.


The evaluation suite, implemented in this research, aims to measure how far the distributed system results (i.e., recommendations) deviate from the single partition results. This difference is computed by metrics used to benchmark recommendations systems. In the evaluation stage, various test scenarios are investigated based on the choice of the data fusion strategy, partitioning method, and the number of partitions. An evaluation pipeline is introduced to simulate all the possible scenarios, which can then be assessed to one another to find the superior choice of method.
